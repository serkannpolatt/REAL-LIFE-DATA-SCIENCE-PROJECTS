{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text mining (nlp) with python/python ile metin madenciliği (nlp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Introduction/Giriş*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code examples to get you started with Python for Natural Language Processing (NLP) / Text Mining.  \n",
    "\n",
    "In the large scheme of things there are roughly 4 steps:  \n",
    "\n",
    "1. Identify a data source  \n",
    "2. Gather the data  \n",
    "3. Process the data  \n",
    "4. Analyze the data  \n",
    "\n",
    "This notebook only discusses step 3 and 4. If you want to learn more about step 2 see my [Python tutorial](https://github.com/TiesdeKok/LearnPythonforResearch). \n",
    "\n",
    "#### ------------------------------------------------------------\n",
    "\n",
    "Bu not defteri, Doğal Dil İşleme (NLP) / Metin Madenciliği için Python'a başlamanıza yardımcı olacak kod örnekleri içerir.\n",
    "\n",
    "Geniş şemada kabaca 4 adım vardır:\n",
    "\n",
    "1. Bir veri kaynağı belirleyin\n",
    "2. Verileri toplayın\n",
    "3. Verileri işleyin\n",
    "4. Verileri analiz edin\n",
    "\n",
    "Bu not defteri yalnızca 3. ve 4. adımı ele almaktadır. 2. adım hakkında daha fazla bilgi edinmek istiyorsanız [Python eğitimime](https://github.com/TiesdeKok/LearnPythonforResearch) bakın."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: companion slides\n",
    "\n",
    "## Not: tamamlayıcı slaytlar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was designed to accompany a PhD course session on NLP techniques in Accounting Research.  \n",
    "The slides of this session are publically availabe here: [Slides](http://www.tiesdekok.com/AccountingNLP_Slides/)\n",
    "\n",
    "### --------------------------------------------\n",
    "\n",
    "Bu defter, Muhasebe Araştırmalarında NLP teknikleri üzerine bir doktora kursu oturumuna eşlik etmek üzere tasarlanmıştır.\n",
    "\n",
    "Bu oturumun slaytları şu adreste herkesin kullanımına sunulmuştur: [Slaytlar](http://www.tiesdekok.com/AccountingNLP_Slides/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Elements / topics that are discussed in this notebook:*\n",
    "# *Bu not defterinde ele alınan unsurlar / konular:*\n",
    "\n",
    "\n",
    "\n",
    "<img style=\"float: left\" src=\"https://i.imgur.com/c3aCZLA.png\" width=\"50%\" /> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Table of Contents/İçindekiler*  <a id='toc'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Primer on NLP tools](#tool_primer)     \n",
    "* [Process + Clean text](#proc_clean)   \n",
    "    * [Normalization](#normalization)\n",
    "        * [Deal with unwanted characters](#unwanted_char)\n",
    "        * [Sentence segmentation](#sentence_seg)   \n",
    "        * [Word tokenization](#word_token)\n",
    "        * [Lemmatization & Stemming](#lem_and_stem) \n",
    "    * [Language modeling](#lang_model) \n",
    "        * [Part-of-Speech tagging](#pos_tagging) \n",
    "        * [Uni-Gram & N-Grams](#n_grams) \n",
    "        * [Stop words](#stop_words) \n",
    "* [Direct feature extraction](#feature_extract) \n",
    "    * [Feature search](#feature_search) \n",
    "        * [Entity recognition](#entity_recognition) \n",
    "        * [Pattern search](#pattern_search) \n",
    "    * [Text evaluation](#text_eval) \n",
    "        * [Language](#language) \n",
    "        * [Dictionary counting](#dict_counting) \n",
    "        * [Readability](#readability) \n",
    "* [Represent text numerically](#text_numerical) \n",
    "    * [Bag of Words](#bows) \n",
    "        * [TF-IDF](#tfidf) \n",
    "    * [Word Embeddings](#word_embed) \n",
    "        * [Spacy](#spacyEmbedding)\n",
    "        * [Word2Vec](#Word2Vec) \n",
    "* [Statistical models](#stat_models) \n",
    "    * [\"Traditional\" machine learning](#trad_ml) \n",
    "        * [Supervised](#trad_ml_supervised) \n",
    "            * [Naïve Bayes](#trad_ml_supervised_nb) \n",
    "            * [Support Vector Machines (SVM)](#trad_ml_supervised_svm) \n",
    "        * [Unsupervised](#trad_ml_unsupervised) \n",
    "            * [Latent Dirichilet Allocation (LDA)](#trad_ml_unsupervised_lda) \n",
    "            * [pyLDAvis](#trad_ml_unsupervised_pyLDAvis) \n",
    "* [Model Selection and Evaluation](#trad_ml_eval) \n",
    "* [Neural Networks](#nn_ml)\n",
    "\n",
    "##### --------------------------------------------------------------\n",
    "\n",
    "* [NLP araçlarında astar](#tool_primer)\n",
    "* [İşle + Metni temizle](#proc_clean)\n",
    "    * [Normalleştirme](#normalleştirme)\n",
    "        * [İstenmeyen karakterlerle uğraşın](#unwanted_char)\n",
    "        * [Cümle segmentasyonu](#sentence_seg)\n",
    "        * [Kelime belirleme](#word_token)\n",
    "        * [Lemmatizasyon ve Saplama](#lem_and_stem)\n",
    "    * [Dil modelleme](#lang_model)\n",
    "        * [Konuşma Bölümü etiketleme](#pos_tagging)\n",
    "        * [Tek Gram ve N-Gram](#n_gram)\n",
    "        * [Kelimeleri durdur](#stop_words)\n",
    "* [Doğrudan özellik çıkarma](#feature_extract)\n",
    "    * [Özellik arama](#feature_search)\n",
    "        * [Varlık tanıma](#entity_recognition)\n",
    "        * [Kalıp arama](#pattern_search)\n",
    "    * [Metin değerlendirmesi](#text_eval)\n",
    "        * [Dil](#dil)\n",
    "        * [Sözlük sayımı](#dict_counting)\n",
    "        * [Okunabilirlik](#okunabilirlik)\n",
    "* [Metni sayısal olarak göster](#text_numerical)\n",
    "    * [Kelime Torbası](#yaylar)\n",
    "        * [TF-IDF](#tfidf)\n",
    "    * [Kelime Gömmeleri](#word_embed)\n",
    "        * [Spacy](#spacyGömme)\n",
    "        * [Word2Vec](#Word2Vec)\n",
    "* [İstatistiksel modeller](#stat_models)\n",
    "    * [\"Geleneksel\" makine öğrenimi](#trad_ml)\n",
    "        * [Denetimli](#trad_ml_denetimli)\n",
    "            * [Naïve Bayes](#trad_ml_supervised_nb)\n",
    "            * [Destek Vektör Makineleri (SVM)](#trad_ml_supervised_svm)\n",
    "        * [Denetimsiz](#trad_ml_undenetimsiz)\n",
    "            * [Gizli Dirichilet Tahsisi (LDA)](#trad_ml_unsupervised_lda)\n",
    "            * [pyLDAvis](#trad_ml_unsupervised_pyLDAvis)\n",
    "* [Model Seçimi ve Değerlendirmesi](#trad_ml_eval)\n",
    "* [Sinir Ağları](#nn_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline;\">Primer on NLP tools</span><a id='tool_primer'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many tools available for NLP purposes.  \n",
    "The code examples below are based on what I personally like to use, it is not intended to be a comprehsnive overview.  \n",
    "\n",
    "Besides build-in Python functionality I will use / demonstrate the following packages:\n",
    "\n",
    "**Standard NLP libraries**:\n",
    "1. `Spacy` \n",
    "2. `NLTK` and the higher-level wrapper `TextBlob`\n",
    "\n",
    "*Note: besides installing the above packages you also often have to download (model) data . Make sure to check the documentation!*\n",
    "\n",
    "**Standard machine learning library**:\n",
    "\n",
    "1. `scikit learn`\n",
    "\n",
    "**Specific task libraries**:\n",
    "\n",
    "There are many, just a couple of examples:\n",
    "\n",
    "1. `pyLDAvis` for visualizing LDA)\n",
    "2. `langdetect` for detecting languages\n",
    "3. `fuzzywuzzy` for fuzzy text matching\n",
    "4. `Gensim` for topic modelling\n",
    "\n",
    "##### -------------------------------------------------------------------------------------\n",
    "\n",
    "NLP amaçları için kullanılabilecek birçok araç vardır.\n",
    "Aşağıdaki kod örnekleri, kişisel olarak kullanmaktan hoşlandığım şeye dayanmaktadır, kapsamlı bir genel bakış olması amaçlanmamıştır.\n",
    "\n",
    "Yerleşik Python işlevselliğinin yanı sıra aşağıdaki paketleri kullanacağım/göstereceğim:\n",
    "\n",
    "**Standart NLP kitaplıkları**:\n",
    "1. \"Boş\"\n",
    "2. \"NLTK\" ve üst düzey sarmalayıcı \"TextBlob\"\n",
    "\n",
    "*Not: Yukarıdaki paketleri kurmanın yanı sıra, genellikle (model) verileri de indirmeniz gerekir. Belgeleri kontrol ettiğinizden emin olun!*\n",
    "\n",
    "**Standart makine öğrenimi kitaplığı**:\n",
    "\n",
    "1. `scikit öğrenmek`\n",
    "\n",
    "**Belirli görev kitaplıkları**:\n",
    "\n",
    "Pek çok, sadece birkaç örnek var:\n",
    "\n",
    "1. LDA'yı görselleştirmek için `pyLDAvis`)\n",
    "2. dilleri tespit etmek için \"langdetect\"\n",
    "3. bulanık metin eşleştirme için \"fuzzywuzzy\"\n",
    "4. Konu modelleme için \"Gensim\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline;\">Get some example data/Bazı örnek veriler alın</span><a id='example_data'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many example datasets available to play around with, see for example this great repository:  \n",
    "https://archive.ics.uci.edu/ml/datasets.php\n",
    "#### -----------------------------------------------------\n",
    "\n",
    "Oynanabilecek birçok örnek veri kümesi vardır, örneğin bu harika depoya bakın:\n",
    "https://archive.ics.uci.edu/ml/datasets.php"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -----------------------------------------------------\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that I will use for most of the examples is the \"Reuter_50_50 Data Set\" that is used for author identification experiments. \n",
    "\n",
    "See the details here: https://archive.ics.uci.edu/ml/datasets/Reuter_50_50  \n",
    "\n",
    "#### -----------------------------------------------------\n",
    "\n",
    "Örneklerin çoğunda kullanacağım veri, yazar tanımlama deneyleri için kullanılan \"Reuter_50_50 Veri Kümesi\" olacaktır.\n",
    "\n",
    "Ayrıntılara buradan bakın: https://archive.ics.uci.edu/ml/datasets/Reuter_50_50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and load the data/Verileri indirin ve yükleyin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't follow what I am doing here? Please see my [Python tutorial](https://github.com/TiesdeKok/LearnPythonforResearch) (although the `zipfile` and `io` operations are not very relevant).\n",
    "\n",
    "Burada ne yaptığımı takip edemiyor musun? Lütfen [Python öğreticime](https://github.com/TiesdeKok/LearnPythonforResearch) bakın (\"zipfile\" ve \"io\" işlemleri pek alakalı olmasa da)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io, os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* for `tqdm` to work in JupyterLab you need to install the `@jupyter-widgets/jupyterlab-manager` using the puzzle icon in the left side bar. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Download and extract the zip file with the data *\n",
    "\n",
    "* Verileri içeren zip dosyasını indirin ve çıkarın *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('C50test'):\n",
    "    r = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip\")\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load the data into memory*\n",
    "\n",
    "*Verileri belleğe yükleyin*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dict = {'test' : 'C50test'}\n",
    "text_dict = {'test' : {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (8.0.6)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (8.11.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets) (4.0.7)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (6.22.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets) (3.0.7)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pyzmq>=20 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.2)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (8.1.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: stack-data in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.1.1)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (305)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6807df40e9ef4aa8ada2964ac7640a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for label, folder in tqdm(folder_dict.items()):\n",
    "    authors = os.listdir(folder)\n",
    "    for author in authors:\n",
    "        text_files = os.listdir(os.path.join(folder, author))\n",
    "        for file in text_files:\n",
    "            with open(os.path.join(folder, author, file), 'r') as text_file:\n",
    "                text_dict[label].setdefault(author, []).append(' '.join(text_file.readlines()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the text comes pre-split per sentence, for the sake of example I undo this through `' '.join(text_file.readlines()`*\n",
    "\n",
    "*Not: metin cümle başına önceden bölünmüş olarak gelir, örnek olsun diye `' '.join(text_file.readlines()`* aracılığıyla bunu geri alıyorum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain\\'s Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers Commission which is due to report before March 24, 1997. The shares fell 6p to 781p on the news.\\n \"The stock is probably dead in the water until March,\" said John Wakley, analyst at Lehman Brothers.  \\n Dermott Carr, an analyst at Nikko said, \"the market is going to hang onto them for the moment but until we get a decision they will be held back.\"\\n Whatever the MMC decides many analysts expect Lang to defer a decision until after the next general election which will be called by May 22.\\n \"They will probably try to defer the decision until after the election. I don\\'t think they want the negative PR of having a large number of people fired,\" said Wakley.  \\n If the deal does not go through, analysts calculate the maximum loss to Bass of 60 million, with most sums centred on the 30-40 million range.\\n \"It\\'s a maxiumum loss of 60 million for Bass if they fail and, unlike Allied, you would have to compare it to the perceived upside of doing the deal,\" said Wakley.\\n Bass said at the time of the deal it would take a one-off charge of 75 million stg for restructuring the combined business, resulting in expected annual cost savings of 90 million stg within three years.  \\n Under the terms of the complex deal, if Bass cannot combine C-T with its own brewing business within 16 months, it has the option to put its whole shareholding to Carlsberg for 110 million stg and Carlsberg has an option to put 15 percent of C-T to Allied Domecq, which would reimburse Bass 30 million stg.\\n Bass is also entitled to receive 50 percent of all profits earnied by C-T until the merger is complete, which should give it some 30-35 million stg in a full year. Carlsberg has agreed to contribute its interests and 20 million stg in exchange for a 20 percent share in the combined Bass Breweries and Carlsberg-Tetley business.\\n C-T was a joint venture between Allied Domecq and Carlsberg formed in 1992 by the merger of their UK brewing and wholesaleing businesses.\\n -- London Newsroom +44 171 542 6437\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline;\">Process + Clean text/İşle + Metni temizle</span><a id='proc_clean'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the text into a NLP representation/Metni bir NLP temsiline dönüştürün"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the text directly, but if want to use packages like `spacy` and `textblob` we first have to convert the text into a corresponding object.  \n",
    "\n",
    "Metni doğrudan kullanabiliriz, ancak \"spacy\" ve \"textblob\" gibi paketleri kullanmak istiyorsak önce metni karşılık gelen bir nesneye dönüştürmemiz gerekir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** depending on the way that you installed the language models you will need to import it differently:\n",
    "\n",
    "**Not:** dil modellerini yükleme şeklinize bağlı olarak, onu farklı şekilde içe aktarmanız gerekir:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from spacy.en import English\n",
    "nlp = English()\n",
    "```\n",
    "OR/VEYA\n",
    "```\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (67.6.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "     ---------------------------------------- 0.0/42.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/42.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.1/42.8 MB 919.0 kB/s eta 0:00:47\n",
      "     ---------------------------------------- 0.1/42.8 MB 1.1 MB/s eta 0:00:41\n",
      "     ---------------------------------------- 0.2/42.8 MB 1.3 MB/s eta 0:00:35\n",
      "     ---------------------------------------- 0.3/42.8 MB 1.4 MB/s eta 0:00:32\n",
      "     ---------------------------------------- 0.4/42.8 MB 1.4 MB/s eta 0:00:30\n",
      "     ---------------------------------------- 0.4/42.8 MB 1.4 MB/s eta 0:00:30\n",
      "     ---------------------------------------- 0.4/42.8 MB 1.4 MB/s eta 0:00:30\n",
      "     ---------------------------------------- 0.4/42.8 MB 1.1 MB/s eta 0:00:40\n",
      "      --------------------------------------- 0.6/42.8 MB 1.2 MB/s eta 0:00:34\n",
      "      --------------------------------------- 0.6/42.8 MB 1.3 MB/s eta 0:00:32\n",
      "      --------------------------------------- 0.8/42.8 MB 1.5 MB/s eta 0:00:28\n",
      "      --------------------------------------- 0.9/42.8 MB 1.5 MB/s eta 0:00:28\n",
      "      --------------------------------------- 1.0/42.8 MB 1.5 MB/s eta 0:00:27\n",
      "      --------------------------------------- 1.1/42.8 MB 1.5 MB/s eta 0:00:28\n",
      "      --------------------------------------- 1.1/42.8 MB 1.5 MB/s eta 0:00:28\n",
      "      --------------------------------------- 1.1/42.8 MB 1.4 MB/s eta 0:00:31\n",
      "     - -------------------------------------- 1.1/42.8 MB 1.4 MB/s eta 0:00:31\n",
      "     - -------------------------------------- 1.4/42.8 MB 1.6 MB/s eta 0:00:27\n",
      "     - -------------------------------------- 1.4/42.8 MB 1.6 MB/s eta 0:00:27\n",
      "     - -------------------------------------- 1.4/42.8 MB 1.6 MB/s eta 0:00:27\n",
      "     - -------------------------------------- 1.4/42.8 MB 1.6 MB/s eta 0:00:27\n",
      "     - -------------------------------------- 1.4/42.8 MB 1.6 MB/s eta 0:00:27\n",
      "     - -------------------------------------- 1.6/42.8 MB 1.5 MB/s eta 0:00:29\n",
      "     - -------------------------------------- 1.6/42.8 MB 1.4 MB/s eta 0:00:30\n",
      "     - -------------------------------------- 1.7/42.8 MB 1.4 MB/s eta 0:00:30\n",
      "     - -------------------------------------- 1.8/42.8 MB 1.4 MB/s eta 0:00:29\n",
      "     - -------------------------------------- 1.8/42.8 MB 1.4 MB/s eta 0:00:30\n",
      "     - -------------------------------------- 1.9/42.8 MB 1.4 MB/s eta 0:00:30\n",
      "     - -------------------------------------- 1.9/42.8 MB 1.4 MB/s eta 0:00:30\n",
      "     - -------------------------------------- 2.0/42.8 MB 1.4 MB/s eta 0:00:30\n",
      "     - -------------------------------------- 2.0/42.8 MB 1.4 MB/s eta 0:00:30\n",
      "     - -------------------------------------- 2.0/42.8 MB 1.4 MB/s eta 0:00:30\n",
      "     -- ------------------------------------- 2.2/42.8 MB 1.4 MB/s eta 0:00:30\n",
      "     -- ------------------------------------- 2.2/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 2.2/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 2.2/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 2.3/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 2.3/42.8 MB 1.3 MB/s eta 0:00:32\n",
      "     -- ------------------------------------- 2.3/42.8 MB 1.3 MB/s eta 0:00:32\n",
      "     -- ------------------------------------- 2.4/42.8 MB 1.3 MB/s eta 0:00:32\n",
      "     -- ------------------------------------- 2.5/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 2.6/42.8 MB 1.3 MB/s eta 0:00:32\n",
      "     -- ------------------------------------- 2.6/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 2.7/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 2.8/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 2.8/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 2.9/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 3.0/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 3.0/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 3.0/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 3.0/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 3.0/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     -- ------------------------------------- 3.1/42.8 MB 1.2 MB/s eta 0:00:33\n",
      "     -- ------------------------------------- 3.2/42.8 MB 1.3 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 3.2/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 3.3/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 3.3/42.8 MB 1.2 MB/s eta 0:00:33\n",
      "     --- ------------------------------------ 3.3/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 3.4/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 3.5/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 3.6/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 3.6/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 3.7/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 3.7/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 3.8/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 3.8/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 3.9/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 3.9/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.0/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.1/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.1/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 4.2/42.8 MB 1.2 MB/s eta 0:00:32\n",
      "     ---- ----------------------------------- 4.3/42.8 MB 1.2 MB/s eta 0:00:31\n",
      "     ---- ----------------------------------- 4.3/42.8 MB 1.2 MB/s eta 0:00:31\n",
      "     ---- ----------------------------------- 4.4/42.8 MB 1.2 MB/s eta 0:00:31\n",
      "     ---- ----------------------------------- 4.5/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     ---- ----------------------------------- 4.5/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     ---- ----------------------------------- 4.6/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     ---- ----------------------------------- 4.7/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     ---- ----------------------------------- 4.7/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     ---- ----------------------------------- 4.8/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     ---- ----------------------------------- 4.9/42.8 MB 1.3 MB/s eta 0:00:31\n",
      "     ---- ----------------------------------- 4.9/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ---- ----------------------------------- 5.0/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ---- ----------------------------------- 5.0/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ---- ----------------------------------- 5.1/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ---- ----------------------------------- 5.2/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ---- ----------------------------------- 5.2/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ---- ----------------------------------- 5.3/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ---- ----------------------------------- 5.3/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ----- ---------------------------------- 5.4/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ----- ---------------------------------- 5.4/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ----- ---------------------------------- 5.5/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ----- ---------------------------------- 5.5/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ----- ---------------------------------- 5.6/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ----- ---------------------------------- 5.7/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ----- ---------------------------------- 5.8/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ----- ---------------------------------- 5.8/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ----- ---------------------------------- 5.8/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ----- ---------------------------------- 5.9/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ----- ---------------------------------- 6.0/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ----- ---------------------------------- 6.1/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ----- ---------------------------------- 6.1/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ----- ---------------------------------- 6.1/42.8 MB 1.3 MB/s eta 0:00:30\n",
      "     ----- ---------------------------------- 6.2/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ----- ---------------------------------- 6.3/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ----- ---------------------------------- 6.3/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ----- ---------------------------------- 6.4/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 6.4/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 6.5/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 6.6/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 6.6/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 6.7/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 6.7/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 6.8/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 6.9/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 6.9/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 7.0/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 7.1/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 7.1/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 7.2/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------ --------------------------------- 7.2/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------ --------------------------------- 7.2/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------ --------------------------------- 7.4/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------ --------------------------------- 7.4/42.8 MB 1.3 MB/s eta 0:00:29\n",
      "     ------ --------------------------------- 7.5/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 7.5/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 7.5/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 7.6/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 7.7/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 7.8/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 7.8/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 7.9/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 7.9/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 8.0/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 8.1/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 8.2/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 8.2/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 8.3/42.8 MB 1.3 MB/s eta 0:00:28\n",
      "     ------- -------------------------------- 8.3/42.8 MB 1.3 MB/s eta 0:00:27\n",
      "     ------- -------------------------------- 8.4/42.8 MB 1.3 MB/s eta 0:00:27\n",
      "     ------- -------------------------------- 8.5/42.8 MB 1.3 MB/s eta 0:00:27\n",
      "     ------- -------------------------------- 8.5/42.8 MB 1.3 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 8.6/42.8 MB 1.3 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 8.7/42.8 MB 1.3 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 8.7/42.8 MB 1.3 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 8.8/42.8 MB 1.3 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 8.9/42.8 MB 1.3 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 8.9/42.8 MB 1.3 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 9.0/42.8 MB 1.3 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 9.1/42.8 MB 1.3 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 9.1/42.8 MB 1.3 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 9.2/42.8 MB 1.3 MB/s eta 0:00:26\n",
      "     -------- ------------------------------- 9.3/42.8 MB 1.3 MB/s eta 0:00:27\n",
      "     -------- ------------------------------- 9.4/42.8 MB 1.3 MB/s eta 0:00:26\n",
      "     -------- ------------------------------- 9.4/42.8 MB 1.3 MB/s eta 0:00:26\n",
      "     -------- ------------------------------- 9.5/42.8 MB 1.3 MB/s eta 0:00:26\n",
      "     -------- ------------------------------- 9.6/42.8 MB 1.3 MB/s eta 0:00:26\n",
      "     --------- ------------------------------ 9.6/42.8 MB 1.3 MB/s eta 0:00:26\n",
      "     --------- ------------------------------ 9.7/42.8 MB 1.3 MB/s eta 0:00:26\n",
      "     --------- ------------------------------ 9.8/42.8 MB 1.3 MB/s eta 0:00:26\n",
      "     --------- ------------------------------ 9.8/42.8 MB 1.3 MB/s eta 0:00:26\n",
      "     --------- ------------------------------ 9.9/42.8 MB 1.3 MB/s eta 0:00:26\n",
      "     --------- ------------------------------ 10.0/42.8 MB 1.3 MB/s eta 0:00:26\n",
      "     --------- ------------------------------ 10.0/42.8 MB 1.3 MB/s eta 0:00:26\n",
      "     --------- ------------------------------ 10.1/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 10.2/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 10.3/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 10.3/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 10.4/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 10.5/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 10.5/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 10.6/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     --------- ------------------------------ 10.7/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     ---------- ----------------------------- 10.7/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     ---------- ----------------------------- 10.8/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     ---------- ----------------------------- 10.9/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     ---------- ----------------------------- 10.9/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     ---------- ----------------------------- 11.0/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     ---------- ----------------------------- 11.1/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     ---------- ----------------------------- 11.2/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     ---------- ----------------------------- 11.2/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     ---------- ----------------------------- 11.3/42.8 MB 1.3 MB/s eta 0:00:25\n",
      "     ---------- ----------------------------- 11.4/42.8 MB 1.3 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 11.4/42.8 MB 1.3 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 11.5/42.8 MB 1.3 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 11.6/42.8 MB 1.3 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 11.7/42.8 MB 1.3 MB/s eta 0:00:24\n",
      "     ---------- ----------------------------- 11.8/42.8 MB 1.3 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 11.8/42.8 MB 1.3 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 11.8/42.8 MB 1.3 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 12.0/42.8 MB 1.3 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 12.0/42.8 MB 1.3 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 12.1/42.8 MB 1.3 MB/s eta 0:00:24\n",
      "     ----------- ---------------------------- 12.2/42.8 MB 1.3 MB/s eta 0:00:23\n",
      "     ----------- ---------------------------- 12.3/42.8 MB 1.3 MB/s eta 0:00:23\n",
      "     ----------- ---------------------------- 12.3/42.8 MB 1.3 MB/s eta 0:00:23\n",
      "     ----------- ---------------------------- 12.4/42.8 MB 1.3 MB/s eta 0:00:23\n",
      "     ----------- ---------------------------- 12.5/42.8 MB 1.4 MB/s eta 0:00:23\n",
      "     ----------- ---------------------------- 12.6/42.8 MB 1.4 MB/s eta 0:00:23\n",
      "     ----------- ---------------------------- 12.6/42.8 MB 1.4 MB/s eta 0:00:23\n",
      "     ----------- ---------------------------- 12.7/42.8 MB 1.4 MB/s eta 0:00:23\n",
      "     ----------- ---------------------------- 12.8/42.8 MB 1.4 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 12.9/42.8 MB 1.4 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 12.9/42.8 MB 1.4 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 13.0/42.8 MB 1.4 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 13.1/42.8 MB 1.4 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 13.1/42.8 MB 1.4 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 13.1/42.8 MB 1.4 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 13.3/42.8 MB 1.4 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 13.3/42.8 MB 1.4 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 13.4/42.8 MB 1.4 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 13.4/42.8 MB 1.4 MB/s eta 0:00:22\n",
      "     ------------ --------------------------- 13.5/42.8 MB 1.4 MB/s eta 0:00:21\n",
      "     ------------ --------------------------- 13.6/42.8 MB 1.4 MB/s eta 0:00:21\n",
      "     ------------ --------------------------- 13.7/42.8 MB 1.4 MB/s eta 0:00:21\n",
      "     ------------ --------------------------- 13.8/42.8 MB 1.4 MB/s eta 0:00:21\n",
      "     ------------ --------------------------- 13.8/42.8 MB 1.4 MB/s eta 0:00:21\n",
      "     ------------- -------------------------- 13.9/42.8 MB 1.4 MB/s eta 0:00:21\n",
      "     ------------- -------------------------- 13.9/42.8 MB 1.4 MB/s eta 0:00:21\n",
      "     ------------- -------------------------- 14.1/42.8 MB 1.4 MB/s eta 0:00:21\n",
      "     ------------- -------------------------- 14.1/42.8 MB 1.4 MB/s eta 0:00:21\n",
      "     ------------- -------------------------- 14.2/42.8 MB 1.4 MB/s eta 0:00:20\n",
      "     ------------- -------------------------- 14.3/42.8 MB 1.4 MB/s eta 0:00:20\n",
      "     ------------- -------------------------- 14.4/42.8 MB 1.4 MB/s eta 0:00:20\n",
      "     ------------- -------------------------- 14.5/42.8 MB 1.4 MB/s eta 0:00:20\n",
      "     ------------- -------------------------- 14.5/42.8 MB 1.4 MB/s eta 0:00:20\n",
      "     ------------- -------------------------- 14.6/42.8 MB 1.4 MB/s eta 0:00:20\n",
      "     ------------- -------------------------- 14.7/42.8 MB 1.4 MB/s eta 0:00:20\n",
      "     ------------- -------------------------- 14.8/42.8 MB 1.4 MB/s eta 0:00:20\n",
      "     ------------- -------------------------- 14.9/42.8 MB 1.4 MB/s eta 0:00:20\n",
      "     ------------- -------------------------- 14.9/42.8 MB 1.4 MB/s eta 0:00:20\n",
      "     ------------- -------------------------- 14.9/42.8 MB 1.4 MB/s eta 0:00:20\n",
      "     -------------- ------------------------- 15.1/42.8 MB 1.5 MB/s eta 0:00:20\n",
      "     -------------- ------------------------- 15.2/42.8 MB 1.4 MB/s eta 0:00:20\n",
      "     -------------- ------------------------- 15.2/42.8 MB 1.4 MB/s eta 0:00:20\n",
      "     -------------- ------------------------- 15.3/42.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -------------- ------------------------- 15.4/42.8 MB 1.4 MB/s eta 0:00:19\n",
      "     -------------- ------------------------- 15.4/42.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -------------- ------------------------- 15.5/42.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -------------- ------------------------- 15.6/42.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -------------- ------------------------- 15.6/42.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -------------- ------------------------- 15.7/42.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -------------- ------------------------- 15.8/42.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -------------- ------------------------- 15.9/42.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -------------- ------------------------- 15.9/42.8 MB 1.5 MB/s eta 0:00:19\n",
      "     -------------- ------------------------- 16.0/42.8 MB 1.5 MB/s eta 0:00:19\n",
      "     --------------- ------------------------ 16.1/42.8 MB 1.5 MB/s eta 0:00:19\n",
      "     --------------- ------------------------ 16.2/42.8 MB 1.5 MB/s eta 0:00:19\n",
      "     --------------- ------------------------ 16.2/42.8 MB 1.5 MB/s eta 0:00:18\n",
      "     --------------- ------------------------ 16.3/42.8 MB 1.5 MB/s eta 0:00:18\n",
      "     --------------- ------------------------ 16.4/42.8 MB 1.5 MB/s eta 0:00:18\n",
      "     --------------- ------------------------ 16.5/42.8 MB 1.5 MB/s eta 0:00:18\n",
      "     --------------- ------------------------ 16.5/42.8 MB 1.5 MB/s eta 0:00:18\n",
      "     --------------- ------------------------ 16.6/42.8 MB 1.5 MB/s eta 0:00:18\n",
      "     --------------- ------------------------ 16.7/42.8 MB 1.5 MB/s eta 0:00:18\n",
      "     --------------- ------------------------ 16.8/42.8 MB 1.5 MB/s eta 0:00:18\n",
      "     --------------- ------------------------ 16.8/42.8 MB 1.5 MB/s eta 0:00:18\n",
      "     --------------- ------------------------ 16.9/42.8 MB 1.5 MB/s eta 0:00:18\n",
      "     --------------- ------------------------ 17.0/42.8 MB 1.5 MB/s eta 0:00:18\n",
      "     --------------- ------------------------ 17.0/42.8 MB 1.5 MB/s eta 0:00:18\n",
      "     --------------- ------------------------ 17.1/42.8 MB 1.5 MB/s eta 0:00:18\n",
      "     ---------------- ----------------------- 17.2/42.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 17.3/42.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 17.3/42.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 17.4/42.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 17.5/42.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 17.6/42.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 17.7/42.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 17.7/42.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 17.8/42.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 17.8/42.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 17.9/42.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 18.0/42.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 18.1/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ---------------- ----------------------- 18.1/42.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ---------------- ----------------------- 18.2/42.8 MB 1.5 MB/s eta 0:00:17\n",
      "     ----------------- ---------------------- 18.3/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 18.3/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 18.3/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 18.5/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 18.5/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 18.5/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 18.6/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 18.6/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 18.7/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 18.8/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 18.9/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 18.9/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 19.0/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 19.0/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 19.1/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 19.2/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 19.2/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ----------------- ---------------------- 19.2/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ------------------ --------------------- 19.3/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ------------------ --------------------- 19.4/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ------------------ --------------------- 19.5/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ------------------ --------------------- 19.5/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ------------------ --------------------- 19.6/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ------------------ --------------------- 19.7/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ------------------ --------------------- 19.7/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ------------------ --------------------- 19.8/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ------------------ --------------------- 19.9/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ------------------ --------------------- 20.0/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------ --------------------- 20.0/42.8 MB 1.5 MB/s eta 0:00:16\n",
      "     ------------------ --------------------- 20.1/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------ --------------------- 20.2/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------ --------------------- 20.3/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------ --------------------- 20.3/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 20.4/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 20.5/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 20.6/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 20.6/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 20.7/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 20.7/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 20.8/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 20.8/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 20.9/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 20.9/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 20.9/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 21.0/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 21.1/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 21.2/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 21.2/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 21.2/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     ------------------- -------------------- 21.3/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 21.4/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 21.4/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 21.5/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 21.6/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 21.7/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 21.7/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 21.8/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 21.9/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 21.9/42.8 MB 1.5 MB/s eta 0:00:15\n",
      "     -------------------- ------------------- 22.0/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------- ------------------- 22.1/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------- ------------------- 22.2/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------- ------------------- 22.2/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------- ------------------- 22.3/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     -------------------- ------------------- 22.4/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 22.5/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 22.5/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 22.6/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 22.7/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 22.7/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 22.8/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 22.9/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 22.9/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 23.0/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 23.1/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 23.2/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 23.2/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 23.3/42.8 MB 1.5 MB/s eta 0:00:14\n",
      "     --------------------- ------------------ 23.3/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     --------------------- ------------------ 23.5/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     --------------------- ------------------ 23.5/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 23.6/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 23.6/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 23.6/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 23.7/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 23.8/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 23.9/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 23.9/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 24.0/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 24.1/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 24.2/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 24.2/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 24.3/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 24.4/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 24.5/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 24.5/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ---------------------- ----------------- 24.6/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 24.7/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 24.8/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 24.9/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 24.9/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 25.0/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 25.0/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 25.0/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 25.0/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 25.1/42.8 MB 1.5 MB/s eta 0:00:12\n",
      "     ----------------------- ---------------- 25.2/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 25.3/42.8 MB 1.5 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 25.3/42.8 MB 1.4 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 25.4/42.8 MB 1.4 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 25.4/42.8 MB 1.4 MB/s eta 0:00:13\n",
      "     ----------------------- ---------------- 25.5/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ----------------------- ---------------- 25.6/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ----------------------- ---------------- 25.7/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 25.7/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 25.8/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 25.9/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 25.9/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 25.9/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 26.0/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 26.1/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 26.1/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 26.2/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 26.3/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 26.4/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 26.4/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 26.5/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 26.6/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 26.6/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------ --------------- 26.6/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 26.7/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 26.8/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 26.9/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 26.9/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 27.0/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 27.1/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 27.1/42.8 MB 1.4 MB/s eta 0:00:12\n",
      "     ------------------------- -------------- 27.2/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     ------------------------- -------------- 27.3/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     ------------------------- -------------- 27.4/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     ------------------------- -------------- 27.4/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     ------------------------- -------------- 27.5/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     ------------------------- -------------- 27.6/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     ------------------------- -------------- 27.7/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     ------------------------- -------------- 27.7/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 27.8/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 27.9/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 27.9/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 28.0/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 28.1/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 28.1/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 28.2/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 28.3/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 28.3/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 28.4/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 28.5/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 28.6/42.8 MB 1.4 MB/s eta 0:00:11\n",
      "     -------------------------- ------------- 28.6/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     -------------------------- ------------- 28.7/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     -------------------------- ------------- 28.7/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     -------------------------- ------------- 28.8/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 28.9/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 28.9/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 29.0/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 29.1/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 29.2/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 29.2/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 29.3/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 29.4/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 29.4/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 29.5/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 29.5/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 29.6/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 29.7/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 29.8/42.8 MB 1.4 MB/s eta 0:00:10\n",
      "     --------------------------- ------------ 29.9/42.8 MB 1.4 MB/s eta 0:00:09\n",
      "     --------------------------- ------------ 29.9/42.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 30.0/42.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 30.1/42.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 30.2/42.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 30.2/42.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 30.3/42.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 30.4/42.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 30.5/42.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 30.6/42.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 30.6/42.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 30.7/42.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 30.8/42.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 30.9/42.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---------------------------- ----------- 30.9/42.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ----------------------------- ---------- 31.0/42.8 MB 1.5 MB/s eta 0:00:09\n",
      "     ----------------------------- ---------- 31.1/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 31.1/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 31.2/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 31.3/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 31.3/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 31.4/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 31.5/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 31.6/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 31.6/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 31.7/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 31.8/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 31.9/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 31.9/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ----------------------------- ---------- 32.0/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 32.1/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 32.2/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 32.2/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 32.3/42.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ------------------------------ --------- 32.4/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 32.5/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 32.5/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 32.6/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 32.7/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 32.7/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 32.8/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 32.9/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 33.0/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 33.0/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------ --------- 33.1/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 33.2/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 33.2/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 33.2/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 33.4/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 33.4/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 33.5/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 33.5/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 33.6/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 33.7/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 33.8/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 33.8/42.8 MB 1.5 MB/s eta 0:00:07\n",
      "     ------------------------------- -------- 33.9/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     ------------------------------- -------- 34.0/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     ------------------------------- -------- 34.0/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     ------------------------------- -------- 34.2/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 34.2/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 34.3/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 34.4/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 34.4/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 34.5/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 34.6/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 34.7/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 34.7/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 34.8/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 34.9/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 34.9/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 35.0/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 35.0/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 35.1/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 35.1/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 35.2/42.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------------------------- ------- 35.3/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 35.4/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 35.4/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 35.5/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 35.6/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 35.6/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 35.8/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 35.8/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 35.9/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 36.0/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 36.1/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 36.2/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 36.2/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     --------------------------------- ------ 36.3/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 36.4/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 36.4/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 36.5/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 36.6/42.8 MB 1.5 MB/s eta 0:00:05\n",
      "     ---------------------------------- ----- 36.7/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 36.7/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 36.9/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 36.9/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 37.0/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 37.1/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 37.2/42.8 MB 1.6 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 37.2/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 37.2/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 37.4/42.8 MB 1.6 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 37.4/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 37.5/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 37.6/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 37.6/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 37.7/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 37.8/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 37.8/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 37.9/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 37.9/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 38.0/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 38.1/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 38.1/42.8 MB 1.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- ---- 38.2/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ----------------------------------- ---- 38.2/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ----------------------------------- ---- 38.3/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ----------------------------------- ---- 38.4/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ----------------------------------- ---- 38.5/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 38.6/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 38.6/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 38.7/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 38.7/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 38.8/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 38.9/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 39.0/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 39.0/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 39.1/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 39.1/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 39.2/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 39.3/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 39.3/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 39.4/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 39.5/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 39.6/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 39.6/42.8 MB 1.5 MB/s eta 0:00:03\n",
      "     ------------------------------------- -- 39.7/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 39.7/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 39.9/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 39.9/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 40.0/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 40.1/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 40.2/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 40.2/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 40.3/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 40.4/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 40.5/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 40.5/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 40.5/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 40.6/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 40.8/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 40.8/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 40.8/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 40.9/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 41.0/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 41.0/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 41.1/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 41.1/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 41.2/42.8 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 41.3/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 41.4/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 41.5/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 41.5/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 41.6/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 41.7/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  41.8/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  41.8/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  41.9/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  41.9/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.0/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.1/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.1/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.1/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.1/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.3/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.3/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.4/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.4/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.5/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.5/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.6/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.6/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.6/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.7/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.8/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.8/42.8 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 42.8/42.8 MB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from en-core-web-md==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (67.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.4.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all text in the \"test\" sample to a `spacy` `doc` object using `nlp.pipe()`:\n",
    "\n",
    "\"Test\" örneğindeki tüm metni \"nlp.pipe()\" kullanarak bir \"spacy\" \"doc\" nesnesine dönüştürün:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6582f7de5fd34b1d83480decd05b8734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy_text = {}\n",
    "for author, text_list in tqdm(text_dict['test'].items()):\n",
    "    spacy_text[author] = list(nlp.pipe(text_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A note on speed:*  This is slow because we didn't disable any compontents, see this note from the documentation:  \n",
    "> Only apply the pipeline components you need. Getting predictions from the model that you don’t actually need adds up and becomes very inefficient at scale. To prevent this, use the disable keyword argument to disable components you don’t need – either when loading a model, or during processing with nlp.pipe. See the section on disabling pipeline components for more details and examples. [link](https://spacy.io/usage/processing-pipelines#disabling)\n",
    "\n",
    "#### -------------------------------------------------------------\n",
    "\n",
    "*Hızla ilgili bir not:* Herhangi bir bileşeni devre dışı bırakmadığımız için bu yavaştır, belgelerden şu nota bakın:\n",
    "> Yalnızca ihtiyacınız olan boru hattı bileşenlerini uygulayın. Modelden gerçekten ihtiyacınız olmayan tahminler almak, ölçek açısından çok verimsiz hale gelir. Bunu önlemek için, bir model yüklerken veya nlp.pipe ile işlem yaparken ihtiyacınız olmayan bileşenleri devre dışı bırakmak için devre dışı bırak anahtar sözcüğü bağımsız değişkenini kullanın. Daha fazla ayrıntı ve örnek için ardışık düzen bileşenlerini devre dışı bırakma bölümüne bakın. [bağlantı](https://spacy.io/usage/processing-pipelines#disabling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spacy_text['TimFarrand'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply basic `nltk` operations directly to the text so we don't need to convert first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all text in the \"test\" sample to a `TextBlob` object using `TextBlob()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob_text = {}\n",
    "for author, text_list in text_dict['test'].items():\n",
    "    textblob_text[author] = [TextBlob(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textblob.blob.TextBlob"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(textblob_text['TimFarrand'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">Normalization</span><a id='normalization'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text normalization** describes the task of transforming the text into a different (more comparable) form.  \n",
    "\n",
    "This can imply many things, I will show a couple of options below:\n",
    "\n",
    "#### ------------------------------------------\n",
    "\n",
    "**Metin normalleştirme**, metni farklı (daha karşılaştırılabilir) bir forma dönüştürme görevini tanımlar.\n",
    "\n",
    "Bu birçok şeyi ima edebilir, aşağıda birkaç seçenek göstereceğim:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Deal with unwanted characters/İstenmeyen karakterlerle başa çıkma</span><a id='unwanted_char'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will often notice that there are characters that you don't want in your text.  \n",
    "\n",
    "Let's look at this sentence for example:\n",
    "\n",
    "> \"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain\\'s Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\"\n",
    "\n",
    "You notice that there are some `\\` and `\\n` in there. These are used to define how a string should be displayed, if we print this text we get:  \n",
    "\n",
    "#### ---------------------------------------------------------------\n",
    "\n",
    "Metninizde istemediğiniz karakterler olduğunu sık sık fark edeceksiniz.\n",
    "\n",
    "Örneğin şu cümleye bakalım:\n",
    "\n",
    "> \"Biracılık-eğlence grubu Bass Plc'deki hisselerin, İngiltere'nin Ticaret ve Sanayi sekreteri Ian Lang'in bira üreticisi Carlsberg-Tetley ile önerilen birleşmeye izin verip vermeyeceğine karar verene kadar tutulacağını söylediler, dedi analistler.\\n Daha önce Lang duyurdu Bas anlaşması, Tekeller ve Birleşmeler olarak anılacaktır\"\n",
    "\n",
    "Orada bazı `\\` ve `\\n` olduğunu fark ettiniz. Bunlar, bir dizenin nasıl görüntülenmesi gerektiğini tanımlamak için kullanılır, eğer bu metni yazdırırsak şunu elde ederiz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\n",
      " Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\n"
     ]
    }
   ],
   "source": [
    "print(text_dict['test']['TimFarrand'][0][:298])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These special characters can cause problems in our analyses (and can be hard to debug if you are using `print` statements to inspect the data).\n",
    "\n",
    "**So how do we remove them?**\n",
    "\n",
    "###### ------------------------------------------------------------------------------------\n",
    "\n",
    "Bu özel karakterler, analizlerimizde sorunlara neden olabilir (ve verileri incelemek için 'yazdır' ifadeleri kullanıyorsanız hata ayıklaması zor olabilir).\n",
    "\n",
    "**Peki bunları nasıl kaldıracağız?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases it is sufficient to simply use the `.replace()` function:\n",
    "\n",
    "Çoğu durumda `.replace()` işlevini kullanmak yeterlidir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts. Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:298].replace('\\n', '').replace('\\\\', '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, however, the problem arrises because of encoding / decoding problems.  \n",
    "\n",
    "In those cases you can usually do something like:  \n",
    "\n",
    "### --------------------------------\n",
    "\n",
    "Ancak bazen, kodlama / kod çözme sorunları nedeniyle sorun ortaya çıkar.\n",
    "\n",
    "Bu durumlarda genellikle şöyle bir şey yapabilirsiniz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is some π text that has to be cleaned…! it's difficult to deal with!\n",
      "b\"This is some  text that has to be cleaned! it's difficult to deal with!\"\n"
     ]
    }
   ],
   "source": [
    "problem_sentence = 'This is some \\u03c0 text that has to be cleaned\\u2026! it\\u0027s difficult to deal with!'\n",
    "print(problem_sentence)\n",
    "print(problem_sentence.encode().decode('unicode_escape').encode('ascii','ignore'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative that is better at preserving the unicode characters would be to use `unidecode`\n",
    "\n",
    "Unicode karakterleri korumada daha iyi bir alternatif, \"unidecode\" kullanmaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "     ---------------------------------------- 0.0/235.9 kB ? eta -:--:--\n",
      "     ----- --------------------------------- 30.7/235.9 kB 1.3 MB/s eta 0:00:01\n",
      "     ------ ------------------------------ 41.0/235.9 kB 393.8 kB/s eta 0:00:01\n",
      "     ----------------- ------------------ 112.6/235.9 kB 930.9 kB/s eta 0:00:01\n",
      "     --------------------- -------------- 143.4/235.9 kB 944.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 235.9/235.9 kB 1.0 MB/s eta 0:00:00\n",
      "Installing collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "王玉\n"
     ]
    }
   ],
   "source": [
    "print('\\u738b\\u7389')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wang Yu '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unidecode.unidecode(u\"\\u738b\\u7389\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is some p text that has to be cleaned...! it's difficult to deal with!\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unidecode.unidecode(problem_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Sentence segmentation/Cümle segmentasyonu</span><a id='sentence_seg'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation refers to the task of splitting up the text by sentence.  \n",
    "\n",
    "You could do this by splitting on the `.` symbol, but dots are used in many other cases as well so it is not very robust:\n",
    "\n",
    "### -----------------------------------------\n",
    "\n",
    "Cümle bölümleme, metni cümleye göre bölme görevini ifade eder.\n",
    "\n",
    "Bunu `.` sembolünü bölerek yapabilirsiniz, ancak diğer birçok durumda noktalar da kullanılır, bu nedenle çok sağlam değildir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts\",\n",
       " '\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers Commission which is due to report before March 24, 1997',\n",
       " ' The shares fell 6p to 781p on the news',\n",
       " '\\n \"The stock is probably dead in the water until March,\" said John Wakley, analyst at Lehman Brothers',\n",
       " '  \\n Dermott Carr, an analyst at Nikko said, \"the mark']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:550].split('.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to use a more sophisticated implementation such as the one by `Spacy`:\n",
    "\n",
    "\"Spacy\" gibi daha karmaşık bir uygulama kullanmak daha iyidir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_paragraph = spacy_text['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\n",
       "  ,\n",
       " Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers Commission which is due to report before March 24, 1997.,\n",
       " The shares fell 6p to 781p on the news.\n",
       "  ,\n",
       " \"The stock is probably dead in the water until March,\" said John Wakley, analyst at Lehman Brothers.  \n",
       "  ,\n",
       " Dermott Carr, an analyst at Nikko said, \"the market is going to hang onto them for the moment but until we get a decision they will be held back.\"\n",
       "  Whatever the MMC decides many analysts expect Lang to defer a decision until after the next general election which will be called by May 22.\n",
       "  ]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = [s for s in example_paragraph.sents]\n",
    "sentence_list[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the returned object is still a `spacy` object:\n",
    "\n",
    "Döndürülen nesnenin hala bir \"boşluk\" nesnesi olduğuna dikkat edin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence_list[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* `spacy` sentence segmentation relies on the text being capitalized, so make sure you didn't convert it to all lower case before running this operation.\n",
    "\n",
    "*Not:* `boşluklu` cümle segmentasyonu, metnin büyük harfle yazılmasına bağlıdır, bu nedenle bu işlemi çalıştırmadan önce metnin tamamını küçük harfe dönüştürmediğinizden emin olun."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to all texts (for use later on):\n",
    "\n",
    "Tüm metinlere uygula (daha sonra kullanmak için):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f561631eb08b44438fefefcc5ed23cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy_sentences = {}\n",
    "for author, text_list in tqdm(spacy_text.items()):\n",
    "    spacy_sentences[author] = [list(text.sents) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\n",
       "  ,\n",
       " Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers Commission which is due to report before March 24, 1997.,\n",
       " The shares fell 6p to 781p on the news.\n",
       "  ]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_sentences['TimFarrand'][0][:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Word tokenization/Kelime belirteci</span><a id='word_token'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenization means to split the sentence (or text) up into words.\n",
    "\n",
    "Kelime belirteci, cümleyi (veya metni) kelimelere bölmek anlamına gelir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\n",
       " "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][0][0]\n",
    "example_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word is called a `token` in this context (hence `tokenization`), using `spacy`:\n",
    "\n",
    "Bir kelime, bu bağlamda \"belirteç\" olarak adlandırılır (dolayısıyla \"belirteçleştirme\"), \"boşluk\" kullanılarak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares,\n",
       " in,\n",
       " brewing,\n",
       " -,\n",
       " to,\n",
       " -,\n",
       " leisure,\n",
       " group,\n",
       " Bass,\n",
       " Plc,\n",
       " are,\n",
       " likely,\n",
       " to,\n",
       " be,\n",
       " held]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [token for token in example_sentence]\n",
    "token_list[0:15]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Lemmatization & Stemming/Lemmatizasyon ve Stemming</span><a id='lem_and_stem'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases you want to convert a word (i.e. token) into a more general representation.  \n",
    "\n",
    "For example: convert \"car\", \"cars\", \"car's\", \"cars'\" all into the word `car`.\n",
    "\n",
    "This is generally done through lemmatization / stemming (different approaches trying to achieve a similar goal).  \n",
    "\n",
    "#### --------------------------------------------------\n",
    "\n",
    "Bazı durumlarda bir kelimeyi (yani belirteci) daha genel bir temsile dönüştürmek istersiniz.\n",
    "\n",
    "Örneğin: \"araba\", \"arabalar\", \"arabalar\", \"arabalar\" kelimelerini \"araba\" kelimesine dönüştürün.\n",
    "\n",
    "Bu genellikle lemmatizasyon / köklendirme (benzer bir hedefe ulaşmaya çalışan farklı yaklaşımlar) yoluyla yapılır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spacy**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Space offers build-in functionality for lemmatization:\n",
    "\n",
    "Space, lemmatizasyon için yerleşik işlevsellik sunar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['share',\n",
       " 'in',\n",
       " 'brewing',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'leisure',\n",
       " 'group',\n",
       " 'Bass',\n",
       " 'Plc',\n",
       " 'be',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'be',\n",
       " 'hold']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized = [token.lemma_ for token in example_sentence]\n",
    "lemmatized[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the NLTK libary we can also use the more aggressive Porter Stemmer\n",
    "\n",
    "NLTK kütüphanesini kullanarak daha agresif Porter Stemmer'ı da kullanabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['share',\n",
       " 'in',\n",
       " 'brew',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'leisur',\n",
       " 'group',\n",
       " 'bass',\n",
       " 'plc',\n",
       " 'are',\n",
       " 'like',\n",
       " 'to',\n",
       " 'be',\n",
       " 'held']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed = [stemmer.stem(token.text) for token in example_sentence]\n",
    "stemmed[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Original  | Spacy Lemma  | NLTK Stemmer\n",
      "-----------------------------------------\n",
      "    Shares  |       share  |       share\n",
      "        in  |          in  |          in\n",
      "   brewing  |     brewing  |        brew\n",
      "         -  |           -  |           -\n",
      "        to  |          to  |          to\n",
      "         -  |           -  |           -\n",
      "   leisure  |     leisure  |      leisur\n",
      "     group  |       group  |       group\n",
      "      Bass  |        Bass  |        bass\n",
      "       Plc  |         Plc  |         plc\n",
      "       are  |          be  |         are\n",
      "    likely  |      likely  |        like\n",
      "        to  |          to  |          to\n",
      "        be  |          be  |          be\n",
      "      held  |        hold  |        held\n"
     ]
    }
   ],
   "source": [
    "print('  Original  | Spacy Lemma  | NLTK Stemmer')\n",
    "print('-' * 41)\n",
    "for original, lemma, stem in zip(token_list[:15], lemmatized[:15], stemmed[:15]):\n",
    "    print(str(original).rjust(10, ' '), ' | ', str(lemma).rjust(10, ' '), ' | ', str(stem).rjust(10, ' '))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my experience it is usually best to use lemmatization instead of a stemmer. \n",
    "\n",
    "Deneyimlerime göre, genellikle bir saplayıcı yerine lemmatizasyon kullanmak en iyisidir."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">Language modeling/Dil modelleme</span><a id='lang_model'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is inherently structured in complex ways, we can often use some of this underlying structure. \n",
    "\n",
    "Metin doğası gereği karmaşık şekillerde yapılandırılmıştır, bu temel yapının bir kısmını sıklıkla kullanabiliriz."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Part-of-Speech tagging/Konuşma Parçası etiketleme</span><a id='pos_tagging'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech tagging refers to the identification of words as nouns, verbs, adjectives, etc. \n",
    "\n",
    "Konuşma etiketlemenin bir kısmı, sözcüklerin isimler, fiiller, sıfatlar vb. olarak tanımlanmasını ifade eder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Shares, 'NOUN'),\n",
       " (in, 'ADP'),\n",
       " (brewing, 'NOUN'),\n",
       " (-, 'PUNCT'),\n",
       " (to, 'ADP'),\n",
       " (-, 'PUNCT'),\n",
       " (leisure, 'NOUN'),\n",
       " (group, 'NOUN'),\n",
       " (Bass, 'PROPN'),\n",
       " (Plc, 'PROPN')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list = [(token, token.pos_) for token in example_sentence]\n",
    "pos_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Uni-Gram & N-Grams</span><a id='n_grams'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously a sentence is not a random collection of words, the sequence of words has information value.  \n",
    "\n",
    "A simple way to incorporate some of this sequence is by using what is called `n-grams`.  \n",
    "An `n-gram` is nothing more than a a combination of `N` words into one token (a uni-gram token is just one word).  \n",
    "\n",
    "So we can convert `\"Sentence about flying cars\"` into a list of bigrams:\n",
    "\n",
    "> Sentence-about, about-flying, flying-cars  \n",
    "\n",
    "See my slide on N-Grams for a more comprehensive example: [click here](http://www.tiesdekok.com/AccountingNLP_Slides/#14)\n",
    "\n",
    "#### ------------------------------------------------\n",
    "\n",
    "Açıkçası, bir cümle rastgele bir kelime koleksiyonu değildir, kelime dizisinin bilgi değeri vardır.\n",
    "\n",
    "Bu dizinin bazılarını birleştirmenin basit bir yolu, \"n-gram\" denilen şeyi kullanmaktır.\n",
    "Bir \"n-gram\", \"N\" sözcüklerin bir belirteçte birleşiminden başka bir şey değildir (bir uni-gram belirteci yalnızca bir sözcüktür).\n",
    "\n",
    "Böylece `\"Uçan arabalar hakkında cümle\"'yi bir bigram listesine dönüştürebiliriz:\n",
    "\n",
    "> Cümle-hakkında, hakkında-uçan, uçan-arabalar\n",
    "\n",
    "Daha kapsamlı bir örnek için N-Grams ile ilgili slaytıma bakın: [burayı tıklayın](http://www.tiesdekok.com/AccountingNLP_Slides/#14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `NLTK`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are-likely', 'likely-to', 'to-be', 'be-held', 'held-back']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_list = ['-'.join(x) for x in nltk.bigrams([token.text for token in example_sentence])]\n",
    "bigram_list[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_without_punctuation(sen_obj):\n",
    "    return [token.text for token in sen_obj if token.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram(sen_obj, n, sep = '-'):\n",
    "    token_list = tokenize_without_punctuation(sen_obj)\n",
    "    number_of_tokens = len(token_list)\n",
    "    ngram_list = []\n",
    "    for i, token in enumerate(token_list[:-n+1]):\n",
    "        ngram_item = [token_list[i + ii] for ii in range(n)]\n",
    "        ngram_list.append(sep.join(ngram_item))\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shares-in', 'in-brewing', 'brewing-to', 'to-leisure', 'leisure-group']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_ngram(example_sentence, 2)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shares-in-brewing',\n",
       " 'in-brewing-to',\n",
       " 'brewing-to-leisure',\n",
       " 'to-leisure-group',\n",
       " 'leisure-group-Bass']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_ngram(example_sentence, 3)[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Stop words/Kelimeleri durdur</span><a id='stop_words'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on what you are trying to do it is possible that there are many words that don't add any information value to the sentence.  \n",
    "\n",
    "The primary example are stop words.  \n",
    "\n",
    "Sometimes you can improve the accuracy of your model by removing stop words.\n",
    "\n",
    "#### ------------------------------------\n",
    "\n",
    "Ne yapmaya çalıştığınıza bağlı olarak, cümleye herhangi bir bilgi değeri katmayan birçok kelime olabilir.\n",
    "\n",
    "Birincil örnek, durdurma sözcükleridir.\n",
    "\n",
    "Bazen durdurma sözcüklerini kaldırarak modelinizin doğruluğunu artırabilirsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_words = [token for token in example_sentence if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares, brewing, -, -, leisure, group, Bass, Plc, likely, held]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares, in, brewing, -, to, -, leisure, group, Bass, Plc]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note* we can also remove punctuation in the same way:\n",
    "\n",
    "*Not* noktalama işaretlerini de aynı şekilde kaldırabiliriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares, brewing, leisure, group, Bass, Plc, likely, held, Britain, Trade]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token for token in example_sentence if not token.is_stop and token.is_alpha][:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap everything into one function\n",
    "\n",
    "## Her şeyi tek bir işleve toplayın"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic SpaCy text processing function**\n",
    "\n",
    "**Temel SpaCy metin işleme işlevi**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split into sentences\n",
    "2. Apply lemmatizer, remove top words, remove punctuation\n",
    "3. Clean up the sentence using `textacy`\n",
    "\n",
    "#### --------------------------------------------\n",
    "\n",
    "1. Cümlelere ayırın\n",
    "2. Lemmatizer uygulayın, en sık kullanılan kelimeleri kaldırın, noktalama işaretlerini kaldırın\n",
    "3. \"textacy\" kullanarak cümleyi temizleyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_custom(text):\n",
    "    sentences = list(nlp(text, disable=['tagger', 'ner', 'entity_linker', 'textcat', 'entitry_ruler']).sents)\n",
    "    lemmatized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        lemmatized_sentences.append([token.lemma_ for token in sentence if not token.is_stop and token.is_alpha])\n",
    "    return [' '.join(sentence) for sentence in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993ead36da4545598b59d34649b72564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Serkan POLAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "spacy_text_clean = {}\n",
    "for author, text_list in tqdm(text_dict['test'].items()):\n",
    "    lst = []\n",
    "    for text in text_list:\n",
    "        lst.append(process_text_custom(text))\n",
    "    spacy_text_clean[author] = lst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* that this would take quite a long time if we didn't disable some of the components. \n",
    "\n",
    "*Not:* bazı bileşenleri devre dışı bırakmasaydık bu işlem oldukça uzun sürerdi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 53431\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for author, texts in spacy_text_clean.items():\n",
    "    for text in texts:\n",
    "        count += len(text)\n",
    "print('Number of sentences/cümle sayısı:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shares brewing leisure group bass plc likely held britain trade industry secretary ian lang decides allow proposed merge brewer carlsberg tetley said analysts',\n",
       " 'earlier lang announced bass deal referred monoplies mergers commission report march',\n",
       " 'shares fell news']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_text_clean['TimFarrand'][0][:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* the quality of the input text is not great, so the sentence segmentation is also not great (without further tweaking).\n",
    "\n",
    "*Not:* giriş metninin kalitesi çok iyi değil, bu nedenle cümle bölümleri de harika değil (daha fazla ince ayar yapmadan)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline;\">Direct feature extraction/Doğrudan özellik çıkarma</span><a id='feature_extract'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have pre-processed our text into something that we can use for direct feature extraction or to convert it to a numerical representation. \n",
    "\n",
    "Artık metnimizi, doğrudan özellik çıkarma veya sayısal bir temsile dönüştürmek için kullanabileceğimiz bir şeye önceden işledik."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">Feature search/Özellik arama</span><a id='feature_search'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Entity recognition/Varlık tanıma</span><a id='entity_recognition'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful / relevant to extract entities that are mentioned in a piece of text.   \n",
    "\n",
    "SpaCy is quite powerful in extracting entities, however, it doesn't work very well on lowercase text.  \n",
    "\n",
    "Given that \"token.lemma\\_\" removes capitalization I will use `spacy_sentences` for this example.\n",
    "\n",
    "#### --------------------------------------------------------\n",
    "\n",
    "Bir metin parçasında bahsedilen varlıkları çıkarmak genellikle yararlıdır/ilgilidir.\n",
    "\n",
    "SpaCy, varlıkları çıkarmada oldukça güçlüdür, ancak küçük harfli metinlerde pek iyi çalışmaz.\n",
    "\n",
    "\"token.lemma\\_\" büyük harf kullanımını kaldırdığı için bu örnek için \"spacy_sentences\" kullanacağım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The stock is probably dead in the water until March,\" said John Wakley, analyst at Lehman Brothers.  \n",
       " "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][0][3]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(March, 'DATE'), (John Wakley, 'PERSON'), (Lehman Brothers, 'ORG')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, i.label_) for i in nlp(example_sentence.text).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "British pub-to-hotel group Greenalls Plc on Thursday reported a 48 percent rise in profits before exceptional items to 148.7 million pounds ($246.4 million), driven by its acquisition of brewer Boddington in November 1995.\n",
       " "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][4][0]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(British, 'NORP'),\n",
       " (Greenalls Plc, 'ORG'),\n",
       " (Thursday, 'DATE'),\n",
       " (48 percent, 'PERCENT'),\n",
       " (148.7 million pounds, 'MONEY'),\n",
       " ($246.4 million, 'MONEY'),\n",
       " (Boddington, 'GPE'),\n",
       " (November 1995, 'DATE')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, i.label_) for i in nlp(example_sentence.text).ents]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Pattern search/Desen arama</span><a id='pattern_search'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the build-in `re` (regular expression) library you can pattern match nearly anything you want.  \n",
    "\n",
    "I will not go into details about regular expressions but see here for a tutorial:  \n",
    "https://regexone.com/references/python  \n",
    "\n",
    "### ----------------------------------\n",
    "\n",
    "Yerleşik \"re\" (düzenli ifade) kitaplığını kullanarak, neredeyse istediğiniz her şeyi modelle eşleştirebilirsiniz.\n",
    "\n",
    "Düzenli ifadeler hakkında ayrıntılara girmeyeceğim, ancak bir eğitim için buraya bakın:\n",
    "https://regexone.com/references/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**: Use [Pythex.org](https://pythex.org/) to try out your regular expression\n",
    "\n",
    "Example on Pythex: <a href=\"https://pythex.org/?regex=IDNUMBER: (\\d\\d\\d-\\w\\w)&test_string=Ties de Kok (IDNUMBER: 123-AZ). Rest of Text.\" target='_blank'>click here</a>\n",
    "\n",
    "##### ---------------------------------------\n",
    "\n",
    "**İPUCU**: Normal ifadenizi denemek için [Pythex.org](https://pythex.org/) kullanın\n",
    "\n",
    "Pythex ile ilgili örnek: <a href=\"https://pythex.org/?regex=IDNUMBER: (\\d\\d\\d-\\w\\w)&test_string=Ties de Kok (IDNUMBER: 123-AZ). Metin.\" target='_blank'>burayı tıklayın</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_1 = 'Ties de Kok (#IDNUMBER: 123-AZ). Rest of text...'\n",
    "string_2 = 'Philip Joos (#IDNUMBER: 663-BY). Rest of text...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'#IDNUMBER: (\\d\\d\\d-\\w\\w)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123-AZ\n",
      "663-BY\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(pattern, string_1)[0])\n",
    "print(re.findall(pattern, string_2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a sentence contains the word 'million' return True, otherwise return False\n",
    "\n",
    "Bir cümle 'milyon' kelimesini içeriyorsa True, aksi takdirde False döndür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysts forecasts pretax profits range million stg restructuring costs million time\n",
      "restructuring cost million anticipated bulk million stemming closure smaller production plant france\n",
      "cadbury drinks business turn million stg trading profit million half entirely contribution dr pepper\n",
      "campbell estimates uk beverages contribute million stg operating profit million time\n",
      "broadly analysts expect pretty flat performance group confectionery business consensus forecast million stg operating profits\n",
      "average analysts calculate beverages chip trading profits million\n",
      "sale percent stake coca cola amp schweppes beverages ccsb operations coca cola enterprises june million stg analysts want clear statement strategy company\n",
      "far analysts company said shareholders expect return investments emerging markets largest far million russian plant\n",
      "cadbury announced investment million stg building new plant wrocoaw poland joint venture china cost million\n",
      "net debt billion end fall million end result ccsb sale providing acquisitions\n"
     ]
    }
   ],
   "source": [
    "for sen in spacy_text_clean['TimFarrand'][2]:\n",
    "    TERM = 'million'\n",
    "    if re.search('million', sen, flags= re.IGNORECASE):\n",
    "        print(sen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">Text evaluation/Metin değerlendirmesi</span><a id='text_eval'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides feature search there are also many ways to analyze the text as a whole.  \n",
    "\n",
    "Let's, for example, evaluate the following paragraph:\n",
    "\n",
    "### ---------------------------------------\n",
    "\n",
    "Özellik aramanın yanı sıra, metni bir bütün olarak analiz etmenin birçok yolu vardır.\n",
    "\n",
    "Örneğin aşağıdaki paragrafı değerlendirelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soft drinks confectionery group cadbury schweppes plc expected report solid percent rise half profits wednesday faces questions performance soft drink main questions success relaunch brand said mark duffy food manufacturing analyst sbc warburg competitor sprite owned coca cola seen agressive marketing push ranked fastest growing brand cadbury dr pepper analysts forecasts pretax profits range million stg restructuring costs million time dividend pence expected restructuring cost million anticipat'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_paragraph = ' '.join([x for x in spacy_text_clean['TimFarrand'][2]])\n",
    "example_paragraph[:500]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Language/Dil</span><a id='language'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `spacy-langdetect` package it is easy to detect the language of a piece of text\n",
    "\n",
    "Spacy-langdetect paketini kullanarak bir metin parçasının dilini tespit etmek kolaydır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy_langdetect"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading spacy_langdetect-0.1.2-py3-none-any.whl (5.0 kB)\n",
      "Collecting pytest\n",
      "  Downloading pytest-7.3.1-py3-none-any.whl (320 kB)\n",
      "     ---------------------------------------- 0.0/320.5 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/320.5 kB ? eta -:--:--\n",
      "     --- --------------------------------- 30.7/320.5 kB 435.7 kB/s eta 0:00:01\n",
      "     --------- --------------------------- 81.9/320.5 kB 657.6 kB/s eta 0:00:01\n",
      "     ---------- -------------------------- 92.2/320.5 kB 751.6 kB/s eta 0:00:01\n",
      "     ---------- -------------------------- 92.2/320.5 kB 751.6 kB/s eta 0:00:01\n",
      "     ------------ ----------------------- 112.6/320.5 kB 437.6 kB/s eta 0:00:01\n",
      "     ------------------ ----------------- 163.8/320.5 kB 517.2 kB/s eta 0:00:01\n",
      "     --------------------- -------------- 194.6/320.5 kB 562.0 kB/s eta 0:00:01\n",
      "     ------------------------------- ---- 276.5/320.5 kB 710.0 kB/s eta 0:00:01\n",
      "     ------------------------------------ 320.5/320.5 kB 764.2 kB/s eta 0:00:00\n",
      "Collecting langdetect==1.0.7\n",
      "  Downloading langdetect-1.0.7.zip (998 kB)\n",
      "     ---------------------------------------- 0.0/998.1 kB ? eta -:--:--\n",
      "     -- ------------------------------------ 61.4/998.1 kB 1.7 MB/s eta 0:00:01\n",
      "     ---- --------------------------------- 122.9/998.1 kB 1.4 MB/s eta 0:00:01\n",
      "     ------- ------------------------------ 204.8/998.1 kB 1.6 MB/s eta 0:00:01\n",
      "     ---------- --------------------------- 276.5/998.1 kB 1.5 MB/s eta 0:00:01\n",
      "     ------------ ------------------------- 317.4/998.1 kB 1.5 MB/s eta 0:00:01\n",
      "     ---------------- --------------------- 430.1/998.1 kB 1.6 MB/s eta 0:00:01\n",
      "     ----------------- -------------------- 460.8/998.1 kB 1.6 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 491.5/998.1 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------- ----------------- 532.5/998.1 kB 1.3 MB/s eta 0:00:01\n",
      "     ----------------------- -------------- 614.4/998.1 kB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------ ------------- 655.4/998.1 kB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------- ------------ 675.8/998.1 kB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 788.5/998.1 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ----- 860.2/998.1 kB 1.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 870.4/998.1 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------  993.3/998.1 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 998.1/998.1 kB 1.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from langdetect==1.0.7->spacy_langdetect) (1.16.0)\n",
      "Collecting iniconfig\n",
      "  Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from pytest->spacy_langdetect) (0.4.6)\n",
      "Collecting tomli>=1.0.0\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting exceptiongroup>=1.0.0rc8\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from pytest->spacy_langdetect) (23.0)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Using cached pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.7-py3-none-any.whl size=993439 sha256=1bedaccdf1adeab8b659b327ff0012a5fa6d2edd74945cf79240a3212220a9d4\n",
      "  Stored in directory: c:\\users\\serkan polat\\appdata\\local\\pip\\cache\\wheels\\97\\f1\\e4\\8b73f7a0421b132755956892d29b1e764d3e0857a6e92e32fe\n",
      "Successfully built langdetect\n",
      "Installing collected packages: tomli, pluggy, langdetect, iniconfig, exceptiongroup, pytest, spacy_langdetect\n",
      "Successfully installed exceptiongroup-1.1.1 iniconfig-2.0.0 langdetect-1.0.7 pluggy-1.0.0 pytest-7.3.1 spacy_langdetect-0.1.2 tomli-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy_langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_langdetect.spacy_langdetect.LanguageDetector at 0x2271582dab0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.factory('language_detector')\n",
    "def create_language_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "nlp.add_pipe('language_detector')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'en', 'score': 0.9999968207324081}\n"
     ]
    }
   ],
   "source": [
    "print(nlp(example_paragraph)._.language)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Readability/Okunabilirlik</span><a id='readability'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally I'd recommend to calculate the readability metrics by yourself as they don't tend to be that difficult to compute. However, there are packages out there that can help, such as `spacy_readability`\n",
    "\n",
    "Genel olarak, okunabilirlik ölçümlerini hesaplamak o kadar da zor olmadığından, kendi başınıza hesaplamanızı tavsiye ederim. Ancak, \"spasy_readability\" gibi yardımcı olabilecek paketler var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.3.9)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (67.6.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.7.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (7.4.6)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy-readability\n",
      "  Using cached spacy_readability-1.4.1-py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: syllapy<1,>=0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy-readability) (0.7.2)\n",
      "Requirement already satisfied: spacy<3.0,>=2.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy-readability) (2.3.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.0,>=2.0->spacy-readability) (1.0.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.0,>=2.0->spacy-readability) (67.6.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.0,>=2.0->spacy-readability) (1.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.0,>=2.0->spacy-readability) (0.7.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.0,>=2.0->spacy-readability) (0.10.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.0,>=2.0->spacy-readability) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.0,>=2.0->spacy-readability) (2.28.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.0,>=2.0->spacy-readability) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.0,>=2.0->spacy-readability) (3.0.8)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.0,>=2.0->spacy-readability) (1.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.0,>=2.0->spacy-readability) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.0,>=2.0->spacy-readability) (1.23.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.0,>=2.0->spacy-readability) (7.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0,>=2.0->spacy-readability) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0,>=2.0->spacy-readability) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0,>=2.0->spacy-readability) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\serkan polat\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0,>=2.0->spacy-readability) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\serkan polat\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.0,>=2.0->spacy-readability) (0.4.6)\n",
      "Installing collected packages: spacy-readability\n",
      "Successfully installed spacy-readability-1.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --user spacy\n",
    "\n",
    "!pip install --user spacy-readability\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_readability import Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Local\\Temp\\ipykernel_13848\\4253514877.py\", line 4, in <module>\n",
      "    nlp = spacy.load(\"en_core_web_sm\")\n",
      "  File \"c:\\Users\\Serkan POLAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\__init__.py\", line 54, in load\n",
      "  File \"c:\\Users\\Serkan POLAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py\", line 449, in load_model\n",
      "    base_exceptions (dict): Base exceptions.\n",
      "OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1177, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1030, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 960, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 870, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 704, in lines\n",
      "    return self._sd.lines\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\Serkan POLAT\\AppData\\Roaming\\Python\\Python310\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy_readability import Readability\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(Readability())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.412857142857145\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I am some really difficult text to read because I use obnoxiously large words.\")\n",
    "print(doc._.flesch_kincaid_grade_level)\n",
    "print(doc._.smog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual example:** FOG index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syllapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fog(document):\n",
    "    doc = nlp(document, disable=['tagger', 'ner', 'entity_linker', 'textcat', 'entitry_ruler'])\n",
    "    sen_list = list(doc.sents)\n",
    "    num_sen = len(sen_list)\n",
    "\n",
    "    num_words = 0\n",
    "    num_complex_words = 0\n",
    "    for sen_obj in sen_list:\n",
    "        words_in_sen = [token.text for token in sen_obj if token.is_alpha]\n",
    "        num_words += len(words_in_sen)\n",
    "        num_complex  = 0\n",
    "        for word in words_in_sen:\n",
    "            num_syl = syllapy.count(word.lower())\n",
    "            if num_syl > 2:\n",
    "                num_complex += 1\n",
    "        num_complex_words += num_complex\n",
    "        \n",
    "    fog = 0.4 * ((num_words / num_sen) + ((num_complex_words / num_words)*100))\n",
    "    return {'fog' : fog, \n",
    "            'num_sen' : num_sen, \n",
    "            'num_words' : num_words, \n",
    "            'num_complex_words' : num_complex_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Serkan POLAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fog': 149.31494252873563,\n",
       " 'num_sen': 1,\n",
       " 'num_words': 348,\n",
       " 'num_complex_words': 88}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_fog(example_paragraph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text similarity/Metin benzerliği"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `fuzzywuzzy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Serkan POLAT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `spacy`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy can provide a similary score based on the semantic similarity ([link](https://spacy.io/usage/vectors-similarity))\n",
    "\n",
    "Spacy, anlamsal benzerliğe dayalı olarak bir benzerlik puanı sağlayabilir ([link](https://spacy.io/usage/vectors-similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999999967579795"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_1 = nlp(\"fuzzy wuzzy was a bear\")\n",
    "tokens_2 = nlp(\"wuzzy fuzzy was a bear\")\n",
    "\n",
    "tokens_1.similarity(tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5968928111786671"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_1 = nlp(\"Tom believes German cars are the best.\")\n",
    "tokens_2 = nlp(\"Sarah recently mentioned that she would like to go on holiday to Germany.\")\n",
    "\n",
    "tokens_1.similarity(tokens_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Term (dictionary) counting/Terim (sözlük) sayımı</span><a id='dict_counting'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common technique for basic NLP insights is to create simple metrics based on term counts. \n",
    "\n",
    "These are relatively easy to implement.\n",
    "\n",
    "#### -----------------------------------\n",
    "\n",
    "Temel NLP içgörüleri için yaygın bir teknik, terim sayımlarına dayalı basit ölçümler oluşturmaktır.\n",
    "\n",
    "Bunların uygulanması nispeten kolaydır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dictionary = ['soft', 'first', 'most', 'be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft 3\n",
      "first 0\n",
      "most 0\n",
      "be 8\n"
     ]
    }
   ],
   "source": [
    "for word in word_dictionary:\n",
    "    print(word, example_paragraph.count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = ['great', 'agree', 'increase']\n",
    "neg = ['bad', 'disagree', 'decrease']\n",
    "\n",
    "sentence = '''According to the president everything is great, great, \n",
    "and great even though some people might disagree with those statements.'''\n",
    "\n",
    "pos_count = 0\n",
    "for word in pos:\n",
    "    pos_count += sentence.lower().count(word)\n",
    "print(pos_count)\n",
    "\n",
    "neg_count = 0\n",
    "for word in neg:\n",
    "    neg_count += sentence.lower().count(word)\n",
    "print(neg_count)\n",
    "\n",
    "pos_count / (neg_count + pos_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the total number of words is also easy:\n",
    "\n",
    "Toplam kelime sayısını elde etmek de kolaydır:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = len([token for token in nlp(sentence) if token.is_alpha])\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the count per word\n",
    "\n",
    "Kelime başına sayımı da kaydedebiliriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count_dict = {}\n",
    "for word in pos:\n",
    "    pos_count_dict[word] = sentence.lower().count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'great': 3, 'agree': 1, 'increase': 0}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* `.lower()` is actually quite slow, if you have a lot of words / sentences it is recommend to minimize the amount of `.lower()` operations that you have to make."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline;\">Represent text numerically/Metni sayısal olarak temsil etme</span><a id='text_numerical'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">Bag of Words</span><a id='bows'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn includes the `CountVectorizer` and `TfidfVectorizer` function.  \n",
    "\n",
    "For details, see the documentation:  \n",
    "[TF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)  \n",
    "[TFIDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n",
    "*Note 1:* these functions also provide a variety of built-in preprocessing options (e.g. ngrames, remove stop words, accent stripper).\n",
    "\n",
    "*Note 2:* example based on the following website [click here](http://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html)\n",
    "\n",
    "### ----------------------------------------------------------------------\n",
    "\n",
    "Sklearn, \"CountVectorizer\" ve \"TfidfVectorizer\" işlevini içerir.\n",
    "\n",
    "Ayrıntılar için belgelere bakın:\n",
    "[TF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n",
    "[TFIDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n",
    "*Not 1:* bu işlevler, çeşitli yerleşik ön işleme seçenekleri de sağlar (ör. ngramlar, durdurma sözcüklerini kaldır, aksan sıyırıcı).\n",
    "\n",
    "*Not 2:* aşağıdaki web sitesine dayanan örnek [burayı tıklayın](http://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">TF-IDF</span><a id='tfidf'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = transformer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78528828 0.         0.         0.6191303  0.         0.        ]\n",
      "[0.         0.47380449 0.         0.         0.47380449 0.74230628]\n",
      "[0.         0.53256952 0.         0.65782931 0.53256952 0.        ]\n",
      "[0.         0.36626037 0.57381765 0.         0.73252075 0.        ]\n"
     ]
    }
   ],
   "source": [
    "for doc_vector in tfidf.toarray():\n",
    "    print(doc_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More elaborate example:\n",
    "### Daha ayrıntılı örnek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_paragraphs = []\n",
    "for author, value in spacy_text_clean.items():\n",
    "    for article in value:\n",
    "        clean_paragraphs.append(' '.join([x for x in article]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_large = transformer.fit_transform(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors: 2500\n",
      "Number of words in dictionary: 27743\n"
     ]
    }
   ],
   "source": [
    "print('Number of vectors/vektör sayısı:', len(tfidf_large.toarray()))\n",
    "print('Number of words in dictionary/Sözlükteki kelime sayısı:', len(tfidf_large.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2500x27743 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 446636 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">Word Embeddings</span><a id='word_embed'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Spacy</span><a id='spacyEmbedding'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `en_core_web_lg` language model comes with GloVe vectors trained on the Common Crawl dataset ([link](https://spacy.io/models/en#en_core_web_lg))\n",
    "\n",
    "`en_core_web_lg` dil modeli, Common Crawl veri setinde eğitilmiş GloVe vektörleriyle birlikte gelir ([link](https://spacy.io/models/en#en_core_web_lg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The True 76.91735 False\n",
      "Dutch True 45.816505 False\n",
      "word True 60.706367 False\n",
      "for True 69.12914 False\n",
      "peanut True 30.872149 False\n",
      "butter True 45.09008 False\n",
      "is True 110.41255 False\n",
      "pindakaas False 0.0 True\n",
      "did True 70.34003 False\n",
      "you True 70.9396 False\n",
      "know True 50.171894 False\n",
      "that True 57.417362 False\n",
      "This True 62.56213 False\n",
      "is True 110.41255 False\n",
      "a True 112.98545 False\n",
      "typpo False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(\"The Dutch word for peanut butter is 'pindakaas', did you know that? This is a typpo.\")\n",
    "\n",
    "for token in tokens:\n",
    "    if token.is_alpha:\n",
    "        print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token: \"Car\" has the following vector (dimension: 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5.6920e+00, -3.2402e+00, -3.0820e+00, -6.7902e-01, -9.6719e-01,\n",
       "       -2.9792e+00,  2.8736e+00, -4.9361e+00, -7.3910e-01,  4.0223e+00,\n",
       "       -5.3932e+00, -1.3357e+00, -4.5541e+00, -1.4588e+00, -7.1353e+00,\n",
       "        3.4909e+00,  2.6185e+00,  1.9497e+00, -4.6816e-01,  2.7521e+00,\n",
       "       -1.5615e+00,  1.1734e+00,  9.5472e-01,  1.1160e-01,  6.9507e+00,\n",
       "        1.2640e+00, -3.9840e+00, -6.4382e+00, -3.2300e+00, -3.0197e+00,\n",
       "        3.2735e+00,  3.3488e+00, -6.4635e-03,  7.1386e+00, -2.0421e+00,\n",
       "        6.4661e+00,  2.1496e-01,  2.7396e+00,  6.5596e-01, -5.7375e+00,\n",
       "        4.3028e+00, -2.8573e-01, -1.1799e+00,  9.6603e+00, -7.9194e+00,\n",
       "       -2.9292e+00, -1.6170e+00,  8.6791e+00,  1.3318e+00, -2.1795e+00,\n",
       "       -1.5050e+00,  1.6407e+00,  1.7286e+00,  2.0606e+00,  5.6960e-01,\n",
       "        4.6689e-02,  1.3460e+00, -1.9011e+00,  6.3613e+00,  2.2340e+00,\n",
       "       -6.6167e+00, -2.1176e+00, -6.1828e+00, -1.2253e+00,  3.5796e+00,\n",
       "       -5.7160e+00, -5.0363e+00, -5.2294e+00,  5.5554e+00,  3.1871e+00,\n",
       "       -2.1413e+00,  8.9140e+00,  1.2928e+00, -2.1078e+00,  3.6846e+00,\n",
       "       -2.2555e+00, -1.6932e-01, -4.3043e+00,  1.0265e+00,  4.9433e+00,\n",
       "        2.8744e+00,  8.8683e+00, -7.9246e+00, -2.4277e+00, -4.0798e+00,\n",
       "       -7.9089e+00,  1.6190e+00,  5.0702e+00, -2.9978e+00,  5.0228e-01,\n",
       "        1.9632e+00,  4.0253e+00, -4.1668e+00, -1.3411e+00, -3.9256e+00,\n",
       "        1.2566e+00,  2.9462e+00,  6.4690e+00, -1.3604e+00, -5.9815e-01,\n",
       "        4.7362e-01, -3.7171e+00,  2.5403e-01, -9.4294e-01, -6.6197e-01,\n",
       "        6.8430e+00,  1.4809e+00,  2.3524e+00, -1.1406e+00, -1.8912e+00,\n",
       "       -2.8495e+00, -1.7368e+00, -3.8328e-01, -5.3497e-01, -8.6645e+00,\n",
       "        1.8733e+00, -6.1277e+00, -2.1980e+00,  5.6811e+00,  3.2432e+00,\n",
       "        1.2961e+00,  1.4077e+00,  5.4822e+00,  3.3328e+00, -7.3151e-02,\n",
       "        1.9593e+00, -9.8073e-01, -4.2960e-01,  8.9427e-01, -8.8521e+00,\n",
       "       -3.0134e+00,  4.6185e+00,  1.2684e+00,  3.2412e+00, -2.6162e+00,\n",
       "        5.1337e+00,  4.7740e+00,  1.0153e+01, -2.3392e+00,  1.2055e+01,\n",
       "       -1.2302e+00, -5.3936e+00,  1.6536e+00,  2.9440e+00, -8.5306e+00,\n",
       "        7.6507e+00,  5.1075e+00, -3.9380e+00, -1.0878e+01,  8.9966e+00,\n",
       "        3.4377e+00, -6.2973e+00,  4.6346e+00,  4.1008e+00,  4.6499e+00,\n",
       "       -6.7628e+00,  1.7816e+00, -2.2684e+00,  4.1844e+00, -2.1793e+00,\n",
       "        6.2748e+00,  8.2937e+00,  6.5352e+00, -4.1863e+00, -7.9015e+00,\n",
       "       -2.2440e+00, -8.0883e+00, -2.5856e+00,  3.1732e+00,  2.1027e-01,\n",
       "       -3.9037e+00, -7.2395e-01, -4.8962e-01,  4.3735e-02,  4.9909e-01,\n",
       "        1.1745e+01, -4.6395e+00,  7.5568e+00,  3.8319e-01,  5.1909e+00,\n",
       "        2.3221e+00,  3.5588e+00, -3.6315e+00, -3.6321e+00, -2.5858e+00,\n",
       "        6.3232e+00, -9.1305e-01,  3.4254e+00, -6.5378e+00, -4.6244e+00,\n",
       "        1.3964e+00,  7.6599e-01,  9.5024e+00, -4.3260e+00, -2.6969e+00,\n",
       "       -6.9648e-01, -2.9554e+00,  6.0362e+00,  3.2232e+00, -6.7119e+00,\n",
       "        4.1472e+00,  4.1558e+00,  8.1590e+00,  1.2464e+01, -5.7109e+00,\n",
       "        7.3230e-01, -1.1727e+01, -1.1431e+00,  2.8323e+00, -2.5484e+00,\n",
       "       -5.7615e+00, -3.8417e+00,  2.6846e+00, -2.0380e+00,  2.0864e+00,\n",
       "        5.0111e-01, -1.1421e+00, -7.0963e+00, -3.8340e+00, -7.1248e+00,\n",
       "        9.3480e-01,  7.1644e+00, -6.5595e+00, -4.8598e+00, -2.5558e+00,\n",
       "       -2.1246e+00,  2.4218e+00,  3.8501e+00,  1.6252e+00, -6.8370e+00,\n",
       "        1.9470e+00,  2.8303e+00, -5.6501e-01,  1.9826e+00, -9.0448e-01,\n",
       "        2.3843e+00,  4.3210e-01,  1.6556e-01,  1.6058e+00, -3.6353e-01,\n",
       "        1.0188e+00, -4.6183e+00,  4.3977e+00,  4.9722e+00, -7.0496e-01,\n",
       "       -4.1152e-01, -3.3102e+00,  3.9279e-01, -9.0178e+00, -7.8474e+00,\n",
       "       -7.8380e-01, -9.1742e-01,  4.6109e+00, -8.3930e-01,  1.2777e+00,\n",
       "        1.5654e+00,  1.0852e+00, -9.6158e+00,  5.3141e+00, -5.5757e-01,\n",
       "       -6.6046e-01, -6.2906e-02, -3.3727e+00, -5.5151e+00,  1.1420e+00,\n",
       "        6.5937e-02,  1.0899e-01, -4.7930e+00,  3.1962e+00, -3.9405e+00,\n",
       "       -2.5142e+00, -4.7277e+00,  5.4997e+00,  1.1348e+00,  2.3541e+00,\n",
       "       -6.1509e+00, -3.7454e+00,  3.7982e+00,  2.5531e+00, -5.5086e+00,\n",
       "        5.1855e+00,  3.8404e-01,  5.6458e+00, -7.7311e+00, -4.8688e+00,\n",
       "       -4.4528e+00,  1.4647e-01,  6.5692e-01, -2.1293e+00, -6.0698e-01,\n",
       "       -6.1048e+00, -5.8791e+00, -4.7885e-01,  7.5969e-01,  7.8780e-01,\n",
       "       -5.4799e+00, -1.9992e+00, -5.1731e+00,  1.1141e+00,  1.1680e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = nlp('Car')\n",
    "print('The token: \"{}\" has the following vector (dimension/aşağıdaki vektöre sahiptir (boyut: {})'.format(token.text, len(token.vector)))\n",
    "token.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Word2Vec</span><a id='Word2Vec'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* you might have to run `nltk.download('brown')` to install the NLTK corpus files\n",
    "\n",
    "*Not:* NLTK korpus dosyalarını yüklemek için `nltk.download('brown')` komutunu çalıştırmanız gerekebilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Serkan\n",
      "[nltk_data]     POLAT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "sentences = brown.sents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('brown_model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('brown_model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find words most similar to 'mother':\n",
    "\n",
    "'Anne' kelimesine en çok benzeyen kelimeleri bulun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('father', 0.9800074100494385), ('husband', 0.9679511189460754), ('wife', 0.9455371499061584), ('son', 0.9285951256752014), ('friend', 0.91536945104599), ('nickname', 0.913755476474762), ('voice', 0.9066001176834106), ('brother', 0.8932086229324341), ('addiction', 0.8855394721031189), ('patient', 0.8833369612693787)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"mother\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the odd one out:\n",
    "\n",
    "Garip olanı bulun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pizza\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.doesnt_match(\"pizza pasta garden fries\".split()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve vector representation of the word \"human\"\n",
    "\n",
    "\"İnsan\" kelimesinin vektör temsilini alın"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.07787943e-01,  3.32118362e-01,  5.27413070e-01,  5.32106221e-01,\n",
       "       -5.47175169e-01, -4.80501473e-01,  1.05670822e+00,  1.30387366e+00,\n",
       "       -5.38607657e-01, -6.87516272e-01, -1.76177230e-02, -5.80767393e-01,\n",
       "        5.06372988e-01, -1.08514535e+00,  1.85228974e-01, -6.29287004e-01,\n",
       "        2.00535670e-01, -1.04284286e-01, -7.37063050e-01, -1.06254590e+00,\n",
       "        4.54772562e-01,  1.31828815e-01,  7.16318429e-01,  2.13544697e-01,\n",
       "       -2.55210102e-01,  8.32315758e-02,  1.23568615e-02,  2.72217356e-02,\n",
       "       -8.63463581e-01,  6.19389899e-02,  4.20464873e-01, -4.16138262e-01,\n",
       "        1.09194183e+00, -4.47616041e-01,  9.87596363e-02,  3.75444055e-01,\n",
       "        1.20622888e-02, -1.00792325e+00, -2.46500000e-01,  1.27593264e-01,\n",
       "        1.97680295e-01, -4.25558358e-01,  7.74740040e-01, -4.79565002e-03,\n",
       "        6.24176204e-01,  2.57402778e-01, -4.71468091e-01, -1.62685752e-01,\n",
       "       -3.23666602e-01,  2.41536587e-01,  2.27633834e-01, -4.73424256e-01,\n",
       "       -6.79329515e-01, -3.97763520e-01, -8.15655053e-01, -6.96703494e-01,\n",
       "        1.20573390e+00, -1.07714474e-01, -3.89460504e-01,  2.39176169e-01,\n",
       "        1.77733272e-01,  2.46962458e-01, -1.23470714e-02, -5.14461279e-01,\n",
       "       -8.49004626e-01,  1.01712692e+00,  3.21926266e-01,  9.73756254e-01,\n",
       "       -1.13354826e+00,  5.27697325e-01, -6.58574849e-02,  1.42451674e-01,\n",
       "        2.51393676e-01,  2.18005821e-01,  1.58481669e+00, -4.59256209e-02,\n",
       "        1.13814116e+00,  6.08060420e-01, -6.28678560e-01, -5.73251605e-01,\n",
       "       -1.05181491e+00,  1.75057694e-01, -7.16351926e-01,  2.14744195e-01,\n",
       "       -5.00352323e-01, -8.86876434e-02,  1.05773695e-01, -9.02314088e-04,\n",
       "        1.17332786e-01,  9.15291980e-02,  7.60153055e-01,  1.14213221e-01,\n",
       "        2.05879062e-01, -1.36696339e-01,  4.04437631e-01,  2.43093684e-01,\n",
       "        3.05399269e-01, -8.55252385e-01, -1.75237924e-01, -1.30297855e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['human']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline;\">Statistical models/İstatistiksel modeller</span><a id='stat_models'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">\"Traditional\" machine learning</span><a id='trad_ml'></a> [(to top)](#toc)\n",
    "\n",
    "## <span style=\"text-decoration: underline;\">\"Geleneksel\" makine öğrenimi</span><a id='trad_ml'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library to use for machine learning is scikit-learn ([\"sklearn\"](http://scikit-learn.org/stable/index.html)).\n",
    "\n",
    "Makine öğrenimi için kullanılacak kitaplık scikit-learn'dür ([\"sklearn\"](http://scikit-learn.org/stable/index.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span>Supervised</span><a id='trad_ml_supervised'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the data into a pandas dataframe (so that we can input it easier)\n",
    "### Verileri bir panda veri çerçevesine dönüştürün (böylece daha kolay girebiliriz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = []\n",
    "for author, value in spacy_text_clean.items():\n",
    "    for article in value:\n",
    "        article_list.append((author, ' '.join([x for x in article])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = pd.DataFrame(article_list, columns=['author', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>PeterHumphrey</td>\n",
       "      <td>western countries geared quietly grant asylum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>AaronPressman</td>\n",
       "      <td>commerce department showed unexpected degree f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>PierreTran</td>\n",
       "      <td>sophie ex wall street lawyer shareholder lobby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>SamuelPerry</td>\n",
       "      <td>hewlett packard jumped solidly microsoft windo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>AlexanderSmith</td>\n",
       "      <td>natwest bank admitted thursday multi million p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author                                               text\n",
       "1861   PeterHumphrey  western countries geared quietly grant asylum ...\n",
       "37     AaronPressman  commerce department showed unexpected degree f...\n",
       "1906      PierreTran  sophie ex wall street lawyer shareholder lobby...\n",
       "2076     SamuelPerry  hewlett packard jumped solidly microsoft windo...\n",
       "139   AlexanderSmith  natwest bank admitted thursday multi million p..."
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the sample into a training and test sample/Eğit ve değerlendir işlevi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(article_df.text, article_df.author, test_size=0.20, random_state=3561)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 500\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate function/Eğit ve değerlendir işlevi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple function to train (i.e. fit) and evaluate the model\n",
    "\n",
    "Modeli eğitmek (yani uygun) ve değerlendirmek için basit işlev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Accuracy on training set:\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(\"Accuracy on testing set:\")\n",
    "    print(clf.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Naïve Bayes estimator</span><a id='trad_ml_supervised_nb'></a> [(to top)](#toc)\n",
    "\n",
    "### <span>Naïve Bayes tahmincisi</span><a id='trad_ml_supervised_nb'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define pipeline\n",
    "\n",
    "Ardışık düzen tanımla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english'\n",
    "                            )),\n",
    "        \n",
    "    ('clf', MultinomialNB(alpha = 1,\n",
    "                          fit_prior = True\n",
    "                          )\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and show evaluation stats\n",
    "\n",
    "Değerlendirme istatistiklerini eğitin ve gösterin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.8485\n",
      "Accuracy on testing set:\n",
      "0.714\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       0.90      1.00      0.95         9\n",
      "       AlanCrosby       0.58      0.92      0.71        12\n",
      "   AlexanderSmith       0.86      0.60      0.71        10\n",
      "  BenjaminKangLim       0.75      0.27      0.40        11\n",
      "    BernardHickey       0.75      0.30      0.43        10\n",
      "      BradDorfman       0.80      1.00      0.89         8\n",
      " DarrenSchuettler       0.58      0.78      0.67         9\n",
      "      DavidLawder       1.00      0.60      0.75        10\n",
      "    EdnaFernandes       1.00      0.67      0.80         9\n",
      "      EricAuchard       0.86      0.67      0.75         9\n",
      "   FumikoFujisaki       1.00      1.00      1.00        10\n",
      "   GrahamEarnshaw       0.59      1.00      0.74        10\n",
      " HeatherScoffield       0.83      0.56      0.67         9\n",
      "       JanLopatka       0.27      0.33      0.30         9\n",
      "    JaneMacartney       0.33      0.60      0.43        10\n",
      "     JimGilchrist       0.73      1.00      0.84         8\n",
      "   JoWinterbottom       1.00      0.70      0.82        10\n",
      "         JoeOrtiz       0.82      1.00      0.90         9\n",
      "     JohnMastrini       0.80      0.24      0.36        17\n",
      "     JonathanBirt       0.44      1.00      0.62         8\n",
      "      KarlPenhaul       0.87      1.00      0.93        13\n",
      "        KeithWeir       0.62      0.80      0.70        10\n",
      "   KevinDrawbaugh       0.89      0.80      0.84        10\n",
      "    KevinMorrison       0.33      1.00      0.50         3\n",
      "    KirstinRidley       0.71      0.56      0.63         9\n",
      "KouroshKarimkhany       0.54      0.88      0.67         8\n",
      "        LydiaZajc       0.82      0.90      0.86        10\n",
      "   LynneO'Donnell       0.89      0.73      0.80        11\n",
      "  LynnleyBrowning       0.93      1.00      0.96        13\n",
      "  MarcelMichelson       1.00      0.50      0.67        12\n",
      "     MarkBendeich       0.86      0.55      0.67        11\n",
      "       MartinWolk       0.57      0.80      0.67         5\n",
      "     MatthewBunce       1.00      0.86      0.92        14\n",
      "    MichaelConnor       0.83      0.77      0.80        13\n",
      "       MureDickie       0.50      0.50      0.50        10\n",
      "        NickLouth       0.83      1.00      0.91        10\n",
      "  PatriciaCommins       0.89      0.89      0.89         9\n",
      "    PeterHumphrey       0.42      0.89      0.57         9\n",
      "       PierreTran       0.56      0.83      0.67         6\n",
      "       RobinSidel       1.00      1.00      1.00        12\n",
      "     RogerFillion       1.00      1.00      1.00         8\n",
      "      SamuelPerry       0.78      0.50      0.61        14\n",
      "     SarahDavison       1.00      0.21      0.35        14\n",
      "      ScottHillis       0.44      0.44      0.44         9\n",
      "      SimonCowell       0.90      0.90      0.90        10\n",
      "         TanEeLyn       0.67      0.57      0.62         7\n",
      "   TheresePoletti       0.80      0.73      0.76        11\n",
      "       TimFarrand       1.00      0.77      0.87        13\n",
      "       ToddNissen       0.60      1.00      0.75         9\n",
      "     WilliamKazer       0.00      0.00      0.00        10\n",
      "\n",
      "         accuracy                           0.71       500\n",
      "        macro avg       0.74      0.73      0.70       500\n",
      "     weighted avg       0.77      0.71      0.70       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results/Sonuçları kaydet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_bayes_results.pkl']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'naive_bayes_results.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict out of sample:\n",
    "\n",
    "Örnek dışı tahmin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_y, example_X = y_train[33], X_train[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual author: AaronPressman\n",
      "Predicted author: AaronPressman\n"
     ]
    }
   ],
   "source": [
    "print('Actual author/gerçek yazar:', example_y)\n",
    "print('Predicted author/Tahmin edilen yazar:', clf.predict([example_X])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Support Vector Machines (SVM)</span><a id='trad_ml_supervised_svm'></a> [(to top)](#toc)\n",
    "\n",
    "### <span>Destek Vektör Makineleri (SVM)</span><a id='trad_ml_supervised_svm'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define pipeline\n",
    "\n",
    "Ardışık düzen tanımla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english'\n",
    "                            )),\n",
    "        \n",
    "    ('clf', SVC(kernel='rbf' ,\n",
    "                C=10, gamma=0.3)\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* The SVC estimator is very sensitive to the hyperparameters!\n",
    "\n",
    "*Not:* SVC tahmincisi, hiperparametrelere karşı çok hassastır!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and show evaluation stats\n",
    "\n",
    "Değerlendirme istatistiklerini eğitin ve gösterin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.997\n",
      "Accuracy on testing set:\n",
      "0.828\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       0.80      0.89      0.84         9\n",
      "       AlanCrosby       0.79      0.92      0.85        12\n",
      "   AlexanderSmith       1.00      0.70      0.82        10\n",
      "  BenjaminKangLim       0.57      0.36      0.44        11\n",
      "    BernardHickey       1.00      0.50      0.67        10\n",
      "      BradDorfman       0.88      0.88      0.88         8\n",
      " DarrenSchuettler       1.00      0.89      0.94         9\n",
      "      DavidLawder       1.00      0.60      0.75        10\n",
      "    EdnaFernandes       0.75      1.00      0.86         9\n",
      "      EricAuchard       0.89      0.89      0.89         9\n",
      "   FumikoFujisaki       1.00      1.00      1.00        10\n",
      "   GrahamEarnshaw       0.77      1.00      0.87        10\n",
      " HeatherScoffield       0.90      1.00      0.95         9\n",
      "       JanLopatka       0.57      0.44      0.50         9\n",
      "    JaneMacartney       0.33      0.40      0.36        10\n",
      "     JimGilchrist       0.88      0.88      0.88         8\n",
      "   JoWinterbottom       1.00      0.90      0.95        10\n",
      "         JoeOrtiz       0.82      1.00      0.90         9\n",
      "     JohnMastrini       0.82      0.82      0.82        17\n",
      "     JonathanBirt       0.73      1.00      0.84         8\n",
      "      KarlPenhaul       0.87      1.00      0.93        13\n",
      "        KeithWeir       0.91      1.00      0.95        10\n",
      "   KevinDrawbaugh       0.73      0.80      0.76        10\n",
      "    KevinMorrison       0.60      1.00      0.75         3\n",
      "    KirstinRidley       1.00      0.67      0.80         9\n",
      "KouroshKarimkhany       0.88      0.88      0.88         8\n",
      "        LydiaZajc       1.00      1.00      1.00        10\n",
      "   LynneO'Donnell       0.82      0.82      0.82        11\n",
      "  LynnleyBrowning       1.00      1.00      1.00        13\n",
      "  MarcelMichelson       1.00      0.75      0.86        12\n",
      "     MarkBendeich       0.92      1.00      0.96        11\n",
      "       MartinWolk       0.71      1.00      0.83         5\n",
      "     MatthewBunce       1.00      0.86      0.92        14\n",
      "    MichaelConnor       0.92      0.85      0.88        13\n",
      "       MureDickie       0.55      0.60      0.57        10\n",
      "        NickLouth       0.90      0.90      0.90        10\n",
      "  PatriciaCommins       0.80      0.89      0.84         9\n",
      "    PeterHumphrey       0.62      0.89      0.73         9\n",
      "       PierreTran       0.75      1.00      0.86         6\n",
      "       RobinSidel       1.00      1.00      1.00        12\n",
      "     RogerFillion       1.00      1.00      1.00         8\n",
      "      SamuelPerry       0.76      0.93      0.84        14\n",
      "     SarahDavison       1.00      0.71      0.83        14\n",
      "      ScottHillis       0.67      0.44      0.53         9\n",
      "      SimonCowell       1.00      0.90      0.95        10\n",
      "         TanEeLyn       0.86      0.86      0.86         7\n",
      "   TheresePoletti       0.90      0.82      0.86        11\n",
      "       TimFarrand       0.92      0.85      0.88        13\n",
      "       ToddNissen       0.82      1.00      0.90         9\n",
      "     WilliamKazer       0.20      0.20      0.20        10\n",
      "\n",
      "         accuracy                           0.83       500\n",
      "        macro avg       0.83      0.83      0.82       500\n",
      "     weighted avg       0.84      0.83      0.83       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf_svm, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_results.pkl']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf_svm, 'svm_results.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict out of sample:\n",
    "\n",
    "Örnek dışı tahmin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_y, example_X = y_train[33], X_train[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual author: AaronPressman\n",
      "Predicted author: AaronPressman\n"
     ]
    }
   ],
   "source": [
    "print('Actual author/gerçek yazar:', example_y)\n",
    "print('Predicted author/Tahmin edilen yazar:', clf_svm.predict([example_X])[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span>Model Selection and Evaluation</span><a id='trad_ml_eval'></a> [(to top)](#toc)\n",
    "\n",
    "## <span>Model Seçimi ve Değerlendirmesi</span><a id='trad_ml_eval'></a> [(to top)](#toc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the `TfidfVectorizer` and `SVC()` estimator take a lot of hyperparameters.  \n",
    "\n",
    "It can be difficult to figure out what the best parameters are.\n",
    "\n",
    "We can use `GridSearchCV` to help figure this out.\n",
    "\n",
    "#### ------------------------------------------------------------------------\n",
    "\n",
    "Hem \"TfidfVectorizer\" hem de \"SVC()\" tahmincisi çok sayıda hiperparametre alır.\n",
    "\n",
    "En iyi parametrelerin ne olduğunu anlamak zor olabilir.\n",
    "\n",
    "Bunu anlamaya yardımcı olması için \"GridSearchCV\" kullanabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the options that should be tried out:\n",
    "\n",
    "Öncelikle denenmesi gereken seçenekleri tanımlıyoruz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_search = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "parameters = { 'vect__stop_words': ['english'],\n",
    "                'vect__strip_accents': ['unicode'],\n",
    "              'vect__max_features' : [1500],\n",
    "              'vect__ngram_range': [(1,1), (2,2) ],\n",
    "             'clf__gamma' : [0.2, 0.3, 0.4], \n",
    "             'clf__C' : [8, 10, 12],\n",
    "              'clf__kernel' : ['rbf']\n",
    "             }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run everything:\n",
    "\n",
    "Her şeyi çalıştır:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer()),\n",
       "                                       (&#x27;clf&#x27;, SVC())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__C&#x27;: [8, 10, 12], &#x27;clf__gamma&#x27;: [0.2, 0.3, 0.4],\n",
       "                         &#x27;clf__kernel&#x27;: [&#x27;rbf&#x27;], &#x27;vect__max_features&#x27;: [1500],\n",
       "                         &#x27;vect__ngram_range&#x27;: [(1, 1), (2, 2)],\n",
       "                         &#x27;vect__stop_words&#x27;: [&#x27;english&#x27;],\n",
       "                         &#x27;vect__strip_accents&#x27;: [&#x27;unicode&#x27;]},\n",
       "             scoring=make_scorer(f1_score, average=micro))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer()),\n",
       "                                       (&#x27;clf&#x27;, SVC())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__C&#x27;: [8, 10, 12], &#x27;clf__gamma&#x27;: [0.2, 0.3, 0.4],\n",
       "                         &#x27;clf__kernel&#x27;: [&#x27;rbf&#x27;], &#x27;vect__max_features&#x27;: [1500],\n",
       "                         &#x27;vect__ngram_range&#x27;: [(1, 1), (2, 2)],\n",
       "                         &#x27;vect__stop_words&#x27;: [&#x27;english&#x27;],\n",
       "                         &#x27;vect__strip_accents&#x27;: [&#x27;unicode&#x27;]},\n",
       "             scoring=make_scorer(f1_score, average=micro))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, SVC())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('vect', TfidfVectorizer()),\n",
       "                                       ('clf', SVC())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'clf__C': [8, 10, 12], 'clf__gamma': [0.2, 0.3, 0.4],\n",
       "                         'clf__kernel': ['rbf'], 'vect__max_features': [1500],\n",
       "                         'vect__ngram_range': [(1, 1), (2, 2)],\n",
       "                         'vect__stop_words': ['english'],\n",
       "                         'vect__strip_accents': ['unicode']},\n",
       "             scoring=make_scorer(f1_score, average=micro))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(clf_search, \n",
    "                    param_grid=parameters, \n",
    "                    scoring=make_scorer(f1_score, average='micro'), \n",
    "                    n_jobs=-1\n",
    "                   )\n",
    "grid.fit(X_train, y_train)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* if you are on a powerful (preferably unix system) you can set n_jobs to the number of available threads to speed up the calculation\n",
    "\n",
    "*Not:* güçlü bir (tercihen unix sistemi) kullanıyorsanız, hesaplamayı hızlandırmak için n_jobs'u mevcut iş parçacığı sayısına ayarlayabilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'clf__C': 10, 'clf__gamma': 0.4, 'clf__kernel': 'rbf', 'vect__max_features': 1500, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'} with a score of 0.80\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       0.80      0.89      0.84         9\n",
      "       AlanCrosby       0.79      0.92      0.85        12\n",
      "   AlexanderSmith       1.00      0.70      0.82        10\n",
      "  BenjaminKangLim       0.57      0.36      0.44        11\n",
      "    BernardHickey       1.00      0.50      0.67        10\n",
      "      BradDorfman       0.88      0.88      0.88         8\n",
      " DarrenSchuettler       1.00      0.89      0.94         9\n",
      "      DavidLawder       1.00      0.60      0.75        10\n",
      "    EdnaFernandes       0.80      0.89      0.84         9\n",
      "      EricAuchard       0.89      0.89      0.89         9\n",
      "   FumikoFujisaki       1.00      1.00      1.00        10\n",
      "   GrahamEarnshaw       0.77      1.00      0.87        10\n",
      " HeatherScoffield       0.90      1.00      0.95         9\n",
      "       JanLopatka       0.57      0.44      0.50         9\n",
      "    JaneMacartney       0.36      0.40      0.38        10\n",
      "     JimGilchrist       0.88      0.88      0.88         8\n",
      "   JoWinterbottom       1.00      0.90      0.95        10\n",
      "         JoeOrtiz       0.82      1.00      0.90         9\n",
      "     JohnMastrini       0.82      0.82      0.82        17\n",
      "     JonathanBirt       0.73      1.00      0.84         8\n",
      "      KarlPenhaul       0.87      1.00      0.93        13\n",
      "        KeithWeir       0.83      1.00      0.91        10\n",
      "   KevinDrawbaugh       0.73      0.80      0.76        10\n",
      "    KevinMorrison       0.60      1.00      0.75         3\n",
      "    KirstinRidley       1.00      0.56      0.71         9\n",
      "KouroshKarimkhany       0.88      0.88      0.88         8\n",
      "        LydiaZajc       1.00      1.00      1.00        10\n",
      "   LynneO'Donnell       0.82      0.82      0.82        11\n",
      "  LynnleyBrowning       1.00      1.00      1.00        13\n",
      "  MarcelMichelson       1.00      0.75      0.86        12\n",
      "     MarkBendeich       0.85      1.00      0.92        11\n",
      "       MartinWolk       0.71      1.00      0.83         5\n",
      "     MatthewBunce       1.00      0.86      0.92        14\n",
      "    MichaelConnor       0.92      0.85      0.88        13\n",
      "       MureDickie       0.55      0.60      0.57        10\n",
      "        NickLouth       0.90      0.90      0.90        10\n",
      "  PatriciaCommins       0.80      0.89      0.84         9\n",
      "    PeterHumphrey       0.62      0.89      0.73         9\n",
      "       PierreTran       0.75      1.00      0.86         6\n",
      "       RobinSidel       1.00      1.00      1.00        12\n",
      "     RogerFillion       1.00      1.00      1.00         8\n",
      "      SamuelPerry       0.76      0.93      0.84        14\n",
      "     SarahDavison       1.00      0.71      0.83        14\n",
      "      ScottHillis       0.57      0.44      0.50         9\n",
      "      SimonCowell       1.00      0.90      0.95        10\n",
      "         TanEeLyn       0.86      0.86      0.86         7\n",
      "   TheresePoletti       0.90      0.82      0.86        11\n",
      "       TimFarrand       0.92      0.85      0.88        13\n",
      "       ToddNissen       0.75      1.00      0.86         9\n",
      "     WilliamKazer       0.20      0.20      0.20        10\n",
      "\n",
      "         accuracy                           0.82       500\n",
      "        macro avg       0.83      0.83      0.82       500\n",
      "     weighted avg       0.84      0.82      0.82       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The best parameters are/En iyi parametreler %s with a score of/bir puanla %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "y_true, y_pred = y_test, grid.predict(X_test)\n",
    "print(metrics.classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
